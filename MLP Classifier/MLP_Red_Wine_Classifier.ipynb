{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_Red_Wine_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh3JYCyerV26"
      },
      "source": [
        "# Classifying Red Wine Quality Types using a Multi-Layer Perceptron\n",
        "\n",
        "\n",
        "*   We will aim to build an end-to-end deep learning model for wine quality classification. \n",
        "* However, this will be benchmarked against a model trained on a preprocessed red wine quality dataset that has undergone feature selection using ANOVA.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpE2lqdFOU9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1788bfd9-8edd-4cc7-c716-ac8ea5b8f15b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization, Dense, Dropout\n",
        "from tensorflow.keras.initializers import GlorotNormal\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "!pip install keras-tuner\n",
        "from kerastuner.tuners import Hyperband\n",
        "from kerastuner import HyperModel\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.6/dist-packages (1.0.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.4.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (20.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m6AGXl9rmbJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953d7d9e-1c13-4f1d-f6da-3307a1b67790"
      },
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DmeIjLaryc8"
      },
      "source": [
        "# Load Red Wine Data\n",
        "wine_df = pd.read_csv(\"/content/drive/My Drive/Business Analytics Stuff/BT2101 Group Project/winequality-red.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_fb1eZ-tbYF"
      },
      "source": [
        "# 1. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFuCjZHqzMqj"
      },
      "source": [
        "## 1.1 Preprocessing & Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp24Ruzpr4qp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "bcdf3815-7147-40a8-e7bd-9954e4def52d"
      },
      "source": [
        "# Creating the Binary Target Variable\n",
        "wine_df[\"good_quality\"] = wine_df[\"quality\"] >= 7\n",
        "\n",
        "# Target Variable\n",
        "target = wine_df.loc[:, 'good_quality']\n",
        "target = target.astype(int)  # 1 = good quality & 0 = other quality\n",
        "target\n",
        "\n",
        "# Interaction Terms\n",
        "final_df = wine_df.drop(columns=['quality', 'good_quality']).copy()\n",
        "final_df['total acidity'] = final_df['fixed acidity'] + final_df['volatile acidity']\n",
        "final_df['pH_sulphate'] = final_df['pH'] * final_df['sulphates']\n",
        "\n",
        "# View final_df\n",
        "final_df.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>total acidity</th>\n",
              "      <th>pH_sulphate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>8.10</td>\n",
              "      <td>1.9656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>8.68</td>\n",
              "      <td>2.1760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.9970</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>8.56</td>\n",
              "      <td>2.1190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.9980</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>11.48</td>\n",
              "      <td>1.8328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>8.10</td>\n",
              "      <td>1.9656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.075</td>\n",
              "      <td>13.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>8.06</td>\n",
              "      <td>1.9656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7.9</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.069</td>\n",
              "      <td>15.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0.9964</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.46</td>\n",
              "      <td>9.4</td>\n",
              "      <td>8.50</td>\n",
              "      <td>1.5180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7.3</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.065</td>\n",
              "      <td>15.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0.9946</td>\n",
              "      <td>3.39</td>\n",
              "      <td>0.47</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.95</td>\n",
              "      <td>1.5933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.02</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.073</td>\n",
              "      <td>9.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.9968</td>\n",
              "      <td>3.36</td>\n",
              "      <td>0.57</td>\n",
              "      <td>9.5</td>\n",
              "      <td>8.38</td>\n",
              "      <td>1.9152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>7.5</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.36</td>\n",
              "      <td>6.1</td>\n",
              "      <td>0.071</td>\n",
              "      <td>17.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>0.9978</td>\n",
              "      <td>3.35</td>\n",
              "      <td>0.80</td>\n",
              "      <td>10.5</td>\n",
              "      <td>8.00</td>\n",
              "      <td>2.6800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  ...  total acidity  pH_sulphate\n",
              "0            7.4              0.70  ...           8.10       1.9656\n",
              "1            7.8              0.88  ...           8.68       2.1760\n",
              "2            7.8              0.76  ...           8.56       2.1190\n",
              "3           11.2              0.28  ...          11.48       1.8328\n",
              "4            7.4              0.70  ...           8.10       1.9656\n",
              "5            7.4              0.66  ...           8.06       1.9656\n",
              "6            7.9              0.60  ...           8.50       1.5180\n",
              "7            7.3              0.65  ...           7.95       1.5933\n",
              "8            7.8              0.58  ...           8.38       1.9152\n",
              "9            7.5              0.50  ...           8.00       2.6800\n",
              "\n",
              "[10 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxKeiMNstkC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c81cac9c-2fde-47f3-a1a9-3643f66eb14f"
      },
      "source": [
        "# 13 Features, hence 13 input neurons\n",
        "len(final_df.columns)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO3gqSM2zDEz"
      },
      "source": [
        "# Train and Test sets\n",
        "X_train, X_test_temp, y_train, y_test_temp = train_test_split(final_df, target, test_size=0.40)\n",
        "\n",
        "# Test and Validation sets\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test_temp, y_test_temp, test_size=0.50)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHMeZGgkuTyJ"
      },
      "source": [
        "# 2. End to End Classifier\n",
        "## 2.1 Baseline NN\n",
        "### 2.1.1 Baseline NN Classifier without Tuning or Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MecsRgXnuIcf"
      },
      "source": [
        "# Model\n",
        "def build_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(30, input_dim=13, activation='relu'))\n",
        "  model.add(Dense(60, activation='relu'))\n",
        "  model.add(Dense(30, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqlBeR0fvN79"
      },
      "source": [
        "### Cross validated results\n",
        "Scale values to [-1, 1] instead of [0, 1] for more efficient backpropagation \n",
        "\n",
        "(see https://stats.stackexchange.com/questions/249378/is-scaling-data-0-1-necessary-when-batch-normalization-is-used)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBUATHcvyBKB"
      },
      "source": [
        "# Create Pipeline 1 with Standard Scaler\n",
        "pipe = []\n",
        "pipe.append(('standardize', StandardScaler()))\n",
        "pipe.append(('mlp', KerasClassifier(build_fn=build_model, epochs=30, batch_size=15, verbose=0)))\n",
        "pipeline = Pipeline(pipe)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrwYi_05yx-R"
      },
      "source": [
        "# Train and get Cross Val Accuraccy\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(pipeline, final_df, target ,cv=kfold)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhvvz10V2YMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d82777-99cd-435c-8864-a421be11594b"
      },
      "source": [
        "print(\"Baseline NN Classifier has a mean cross val accuracy score of {:.2f}%\".format(results.mean()*100))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline NN Classifier has a mean cross val accuracy score of 89.56%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjqJnnn90Se8"
      },
      "source": [
        "### Train-Test Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au-ByJv9Vq_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e10c668e-b44b-467f-8ffd-29c01d8673e3"
      },
      "source": [
        "# Get F1-score\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/wrappers/scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKeVWd58-yFy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "eb3f4aa5-bff9-4057-e113-b2aa15c2d921"
      },
      "source": [
        "# Classification report for Baseline NN Classifier \n",
        "pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>weighted avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.940767</td>\n",
              "      <td>0.606061</td>\n",
              "      <td>0.90625</td>\n",
              "      <td>0.773414</td>\n",
              "      <td>0.902066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.954064</td>\n",
              "      <td>0.540541</td>\n",
              "      <td>0.90625</td>\n",
              "      <td>0.747302</td>\n",
              "      <td>0.906250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.947368</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.90625</td>\n",
              "      <td>0.759398</td>\n",
              "      <td>0.903900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>283.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>0.90625</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>320.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    0          1  accuracy   macro avg  weighted avg\n",
              "precision    0.940767   0.606061   0.90625    0.773414      0.902066\n",
              "recall       0.954064   0.540541   0.90625    0.747302      0.906250\n",
              "f1-score     0.947368   0.571429   0.90625    0.759398      0.903900\n",
              "support    283.000000  37.000000   0.90625  320.000000    320.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm_jw43dV57h"
      },
      "source": [
        "### 2.1.2 Baseline NN with Class Weights to correct for Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHyuustW2xki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cb2e234-4ee8-41ad-b9b6-e8ccb95dce62"
      },
      "source": [
        "# Scale & Instantiate Sample Weights\n",
        "weights = {0:1, 1:10}\n",
        "std_scaler = StandardScaler()\n",
        "X_train_scaled = std_scaler.fit_transform(X_train, y_train)\n",
        "X_val_scaled = std_scaler.fit_transform(X_val, y_val)\n",
        "X_test_scaled = std_scaler.fit_transform(X_test, y_test)\n",
        "\n",
        "# Call backs for early stopping\n",
        "my_callbacks = [\n",
        "                EarlyStopping(monitor='val_accuracy', patience=50, mode='max'),\n",
        "                TensorBoard(log_dir='./logs')\n",
        "                ]\n",
        "\n",
        "weighted_classifier = build_model()\n",
        "weighted_classifier.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), class_weight=weights, epochs=200, batch_size=16, callbacks=my_callbacks)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            " 1/60 [..............................] - ETA: 0s - loss: 1.5593 - accuracy: 0.3750WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0066s vs `on_train_batch_end` time: 0.0219s). Check your callbacks.\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 1.2576 - accuracy: 0.3942 - val_loss: 0.7283 - val_accuracy: 0.6344\n",
            "Epoch 2/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 1.0200 - accuracy: 0.7059 - val_loss: 0.6795 - val_accuracy: 0.6625\n",
            "Epoch 3/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.9271 - accuracy: 0.7414 - val_loss: 0.7515 - val_accuracy: 0.6313\n",
            "Epoch 4/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.8682 - accuracy: 0.7424 - val_loss: 0.6793 - val_accuracy: 0.6656\n",
            "Epoch 5/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.8251 - accuracy: 0.7414 - val_loss: 0.6272 - val_accuracy: 0.6875\n",
            "Epoch 6/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.7838 - accuracy: 0.7456 - val_loss: 0.4972 - val_accuracy: 0.7312\n",
            "Epoch 7/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.7582 - accuracy: 0.7591 - val_loss: 0.4247 - val_accuracy: 0.7688\n",
            "Epoch 8/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.7368 - accuracy: 0.7539 - val_loss: 0.4848 - val_accuracy: 0.7469\n",
            "Epoch 9/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.7085 - accuracy: 0.7737 - val_loss: 0.5376 - val_accuracy: 0.7344\n",
            "Epoch 10/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.6842 - accuracy: 0.7873 - val_loss: 0.5077 - val_accuracy: 0.7406\n",
            "Epoch 11/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.6559 - accuracy: 0.8019 - val_loss: 0.6121 - val_accuracy: 0.7188\n",
            "Epoch 12/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.6730 - accuracy: 0.7831 - val_loss: 0.5669 - val_accuracy: 0.7281\n",
            "Epoch 13/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.6313 - accuracy: 0.8269 - val_loss: 0.6231 - val_accuracy: 0.7031\n",
            "Epoch 14/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.6232 - accuracy: 0.8029 - val_loss: 0.5173 - val_accuracy: 0.7469\n",
            "Epoch 15/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5987 - accuracy: 0.8238 - val_loss: 0.6162 - val_accuracy: 0.7219\n",
            "Epoch 16/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.5731 - accuracy: 0.8238 - val_loss: 0.5194 - val_accuracy: 0.7625\n",
            "Epoch 17/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.5568 - accuracy: 0.8269 - val_loss: 0.4665 - val_accuracy: 0.7719\n",
            "Epoch 18/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.5442 - accuracy: 0.8332 - val_loss: 0.5317 - val_accuracy: 0.7750\n",
            "Epoch 19/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.5108 - accuracy: 0.8509 - val_loss: 0.4600 - val_accuracy: 0.7875\n",
            "Epoch 20/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5070 - accuracy: 0.8446 - val_loss: 0.4791 - val_accuracy: 0.7844\n",
            "Epoch 21/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.8665 - val_loss: 0.5359 - val_accuracy: 0.7719\n",
            "Epoch 22/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.4568 - accuracy: 0.8697 - val_loss: 0.4635 - val_accuracy: 0.8156\n",
            "Epoch 23/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.4571 - accuracy: 0.8822 - val_loss: 0.5553 - val_accuracy: 0.7594\n",
            "Epoch 24/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4383 - accuracy: 0.8697 - val_loss: 0.5208 - val_accuracy: 0.7969\n",
            "Epoch 25/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4149 - accuracy: 0.8843 - val_loss: 0.5163 - val_accuracy: 0.7937\n",
            "Epoch 26/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4058 - accuracy: 0.8843 - val_loss: 0.5496 - val_accuracy: 0.7937\n",
            "Epoch 27/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.3757 - accuracy: 0.8895 - val_loss: 0.4925 - val_accuracy: 0.8188\n",
            "Epoch 28/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.3777 - accuracy: 0.8947 - val_loss: 0.5154 - val_accuracy: 0.8281\n",
            "Epoch 29/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.3667 - accuracy: 0.8978 - val_loss: 0.5516 - val_accuracy: 0.7969\n",
            "Epoch 30/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3291 - accuracy: 0.9062 - val_loss: 0.5043 - val_accuracy: 0.8406\n",
            "Epoch 31/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3343 - accuracy: 0.9187 - val_loss: 0.5825 - val_accuracy: 0.8156\n",
            "Epoch 32/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.3310 - accuracy: 0.9062 - val_loss: 0.5597 - val_accuracy: 0.8344\n",
            "Epoch 33/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3085 - accuracy: 0.9155 - val_loss: 0.5560 - val_accuracy: 0.8250\n",
            "Epoch 34/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2873 - accuracy: 0.9166 - val_loss: 0.5718 - val_accuracy: 0.8406\n",
            "Epoch 35/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2615 - accuracy: 0.9312 - val_loss: 0.5419 - val_accuracy: 0.8562\n",
            "Epoch 36/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2708 - accuracy: 0.9239 - val_loss: 0.5723 - val_accuracy: 0.8469\n",
            "Epoch 37/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2605 - accuracy: 0.9239 - val_loss: 0.6074 - val_accuracy: 0.8406\n",
            "Epoch 38/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2787 - accuracy: 0.9228 - val_loss: 0.6622 - val_accuracy: 0.8313\n",
            "Epoch 39/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2351 - accuracy: 0.9343 - val_loss: 0.6652 - val_accuracy: 0.8250\n",
            "Epoch 40/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2226 - accuracy: 0.9374 - val_loss: 0.6774 - val_accuracy: 0.8594\n",
            "Epoch 41/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2211 - accuracy: 0.9374 - val_loss: 0.6853 - val_accuracy: 0.8250\n",
            "Epoch 42/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2068 - accuracy: 0.9406 - val_loss: 0.6700 - val_accuracy: 0.8375\n",
            "Epoch 43/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1878 - accuracy: 0.9499 - val_loss: 0.7164 - val_accuracy: 0.8438\n",
            "Epoch 44/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1719 - accuracy: 0.9541 - val_loss: 0.7226 - val_accuracy: 0.8344\n",
            "Epoch 45/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1647 - accuracy: 0.9552 - val_loss: 0.7693 - val_accuracy: 0.8281\n",
            "Epoch 46/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1867 - accuracy: 0.9437 - val_loss: 0.7680 - val_accuracy: 0.8438\n",
            "Epoch 47/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2186 - accuracy: 0.9468 - val_loss: 0.7475 - val_accuracy: 0.8625\n",
            "Epoch 48/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1730 - accuracy: 0.9531 - val_loss: 0.7945 - val_accuracy: 0.8406\n",
            "Epoch 49/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1989 - accuracy: 0.9426 - val_loss: 0.8232 - val_accuracy: 0.8438\n",
            "Epoch 50/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1603 - accuracy: 0.9552 - val_loss: 0.7441 - val_accuracy: 0.8594\n",
            "Epoch 51/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1273 - accuracy: 0.9635 - val_loss: 0.8023 - val_accuracy: 0.8781\n",
            "Epoch 52/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1714 - accuracy: 0.9489 - val_loss: 0.9107 - val_accuracy: 0.8594\n",
            "Epoch 53/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1544 - accuracy: 0.9604 - val_loss: 0.8301 - val_accuracy: 0.8625\n",
            "Epoch 54/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1280 - accuracy: 0.9708 - val_loss: 0.8655 - val_accuracy: 0.8594\n",
            "Epoch 55/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1182 - accuracy: 0.9729 - val_loss: 0.8304 - val_accuracy: 0.8594\n",
            "Epoch 56/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0995 - accuracy: 0.9760 - val_loss: 0.8982 - val_accuracy: 0.8469\n",
            "Epoch 57/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0978 - accuracy: 0.9771 - val_loss: 0.9215 - val_accuracy: 0.8656\n",
            "Epoch 58/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0974 - accuracy: 0.9739 - val_loss: 0.9351 - val_accuracy: 0.8625\n",
            "Epoch 59/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.9791 - val_loss: 0.9762 - val_accuracy: 0.8594\n",
            "Epoch 60/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0799 - accuracy: 0.9812 - val_loss: 0.9972 - val_accuracy: 0.8594\n",
            "Epoch 61/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0745 - accuracy: 0.9833 - val_loss: 0.9266 - val_accuracy: 0.8750\n",
            "Epoch 62/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0774 - accuracy: 0.9854 - val_loss: 0.9403 - val_accuracy: 0.8781\n",
            "Epoch 63/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1168 - accuracy: 0.9750 - val_loss: 1.0978 - val_accuracy: 0.8188\n",
            "Epoch 64/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1033 - accuracy: 0.9739 - val_loss: 0.9701 - val_accuracy: 0.8625\n",
            "Epoch 65/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0727 - accuracy: 0.9875 - val_loss: 1.0328 - val_accuracy: 0.8719\n",
            "Epoch 66/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0704 - accuracy: 0.9864 - val_loss: 1.0889 - val_accuracy: 0.8438\n",
            "Epoch 67/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0669 - accuracy: 0.9864 - val_loss: 1.0504 - val_accuracy: 0.8750\n",
            "Epoch 68/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0584 - accuracy: 0.9896 - val_loss: 1.0437 - val_accuracy: 0.8750\n",
            "Epoch 69/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9812 - val_loss: 1.0706 - val_accuracy: 0.8656\n",
            "Epoch 70/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0542 - accuracy: 0.9885 - val_loss: 1.0733 - val_accuracy: 0.8687\n",
            "Epoch 71/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9875 - val_loss: 1.0450 - val_accuracy: 0.8531\n",
            "Epoch 72/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.9812 - val_loss: 1.0375 - val_accuracy: 0.8813\n",
            "Epoch 73/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0531 - accuracy: 0.9906 - val_loss: 1.0727 - val_accuracy: 0.8781\n",
            "Epoch 74/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0563 - accuracy: 0.9906 - val_loss: 1.2169 - val_accuracy: 0.8531\n",
            "Epoch 75/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9896 - val_loss: 1.1167 - val_accuracy: 0.8625\n",
            "Epoch 76/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1020 - accuracy: 0.9781 - val_loss: 1.1047 - val_accuracy: 0.8750\n",
            "Epoch 77/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1290 - accuracy: 0.9729 - val_loss: 1.1783 - val_accuracy: 0.8313\n",
            "Epoch 78/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1046 - accuracy: 0.9750 - val_loss: 1.1963 - val_accuracy: 0.8406\n",
            "Epoch 79/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9875 - val_loss: 1.1157 - val_accuracy: 0.8719\n",
            "Epoch 80/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0422 - accuracy: 0.9948 - val_loss: 1.1843 - val_accuracy: 0.8625\n",
            "Epoch 81/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.9927 - val_loss: 1.1584 - val_accuracy: 0.8719\n",
            "Epoch 82/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0367 - accuracy: 0.9927 - val_loss: 1.2014 - val_accuracy: 0.8687\n",
            "Epoch 83/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 0.9937 - val_loss: 1.2142 - val_accuracy: 0.8687\n",
            "Epoch 84/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0677 - accuracy: 0.9875 - val_loss: 1.3360 - val_accuracy: 0.8469\n",
            "Epoch 85/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.9844 - val_loss: 1.5202 - val_accuracy: 0.8125\n",
            "Epoch 86/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5690 - accuracy: 0.9093 - val_loss: 1.2531 - val_accuracy: 0.8031\n",
            "Epoch 87/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.3436 - accuracy: 0.9176 - val_loss: 0.8889 - val_accuracy: 0.8531\n",
            "Epoch 88/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1605 - accuracy: 0.9625 - val_loss: 0.9944 - val_accuracy: 0.8594\n",
            "Epoch 89/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1128 - accuracy: 0.9718 - val_loss: 1.0355 - val_accuracy: 0.8625\n",
            "Epoch 90/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0591 - accuracy: 0.9875 - val_loss: 1.0995 - val_accuracy: 0.8594\n",
            "Epoch 91/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0494 - accuracy: 0.9917 - val_loss: 1.0958 - val_accuracy: 0.8625\n",
            "Epoch 92/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0392 - accuracy: 0.9948 - val_loss: 1.1270 - val_accuracy: 0.8625\n",
            "Epoch 93/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0379 - accuracy: 0.9948 - val_loss: 1.1573 - val_accuracy: 0.8625\n",
            "Epoch 94/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9948 - val_loss: 1.1800 - val_accuracy: 0.8625\n",
            "Epoch 95/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0274 - accuracy: 0.9969 - val_loss: 1.1791 - val_accuracy: 0.8687\n",
            "Epoch 96/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0266 - accuracy: 0.9958 - val_loss: 1.2169 - val_accuracy: 0.8625\n",
            "Epoch 97/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.9969 - val_loss: 1.2126 - val_accuracy: 0.8687\n",
            "Epoch 98/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.9979 - val_loss: 1.2232 - val_accuracy: 0.8719\n",
            "Epoch 99/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.9969 - val_loss: 1.2498 - val_accuracy: 0.8687\n",
            "Epoch 100/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.9969 - val_loss: 1.2859 - val_accuracy: 0.8656\n",
            "Epoch 101/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9958 - val_loss: 1.2322 - val_accuracy: 0.8719\n",
            "Epoch 102/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.9958 - val_loss: 1.2902 - val_accuracy: 0.8719\n",
            "Epoch 103/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.9958 - val_loss: 1.2925 - val_accuracy: 0.8687\n",
            "Epoch 104/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.9969 - val_loss: 1.2898 - val_accuracy: 0.8719\n",
            "Epoch 105/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.9979 - val_loss: 1.3304 - val_accuracy: 0.8687\n",
            "Epoch 106/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.9979 - val_loss: 1.3202 - val_accuracy: 0.8781\n",
            "Epoch 107/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.9969 - val_loss: 1.3176 - val_accuracy: 0.8813\n",
            "Epoch 108/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.9948 - val_loss: 1.3771 - val_accuracy: 0.8750\n",
            "Epoch 109/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.9979 - val_loss: 1.3727 - val_accuracy: 0.8687\n",
            "Epoch 110/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.9948 - val_loss: 1.3795 - val_accuracy: 0.8781\n",
            "Epoch 111/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.9979 - val_loss: 1.3144 - val_accuracy: 0.8594\n",
            "Epoch 112/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.9927 - val_loss: 1.3307 - val_accuracy: 0.8719\n",
            "Epoch 113/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.9958 - val_loss: 1.4273 - val_accuracy: 0.8687\n",
            "Epoch 114/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.9969 - val_loss: 1.4510 - val_accuracy: 0.8594\n",
            "Epoch 115/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.9969 - val_loss: 1.4068 - val_accuracy: 0.8781\n",
            "Epoch 116/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0157 - accuracy: 0.9969 - val_loss: 1.4338 - val_accuracy: 0.8687\n",
            "Epoch 117/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 0.9969 - val_loss: 1.4456 - val_accuracy: 0.8813\n",
            "Epoch 118/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.9958 - val_loss: 1.4559 - val_accuracy: 0.8719\n",
            "Epoch 119/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.9958 - val_loss: 1.5049 - val_accuracy: 0.8750\n",
            "Epoch 120/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.9969 - val_loss: 1.4525 - val_accuracy: 0.8844\n",
            "Epoch 121/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.9958 - val_loss: 1.5577 - val_accuracy: 0.8813\n",
            "Epoch 122/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.9969 - val_loss: 1.5359 - val_accuracy: 0.8656\n",
            "Epoch 123/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0159 - accuracy: 0.9958 - val_loss: 1.5757 - val_accuracy: 0.8844\n",
            "Epoch 124/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.9948 - val_loss: 1.5599 - val_accuracy: 0.8781\n",
            "Epoch 125/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 0.9990 - val_loss: 1.5139 - val_accuracy: 0.8781\n",
            "Epoch 126/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.9969 - val_loss: 1.6019 - val_accuracy: 0.8781\n",
            "Epoch 127/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9990 - val_loss: 1.5555 - val_accuracy: 0.8875\n",
            "Epoch 128/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9969 - val_loss: 1.5941 - val_accuracy: 0.8687\n",
            "Epoch 129/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.9969 - val_loss: 1.5246 - val_accuracy: 0.8813\n",
            "Epoch 130/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0121 - accuracy: 0.9990 - val_loss: 1.5500 - val_accuracy: 0.8875\n",
            "Epoch 131/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0164 - accuracy: 0.9969 - val_loss: 1.5432 - val_accuracy: 0.8813\n",
            "Epoch 132/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0114 - accuracy: 0.9979 - val_loss: 1.5333 - val_accuracy: 0.8813\n",
            "Epoch 133/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0275 - accuracy: 0.9948 - val_loss: 1.7378 - val_accuracy: 0.8562\n",
            "Epoch 134/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.9969 - val_loss: 1.6809 - val_accuracy: 0.8781\n",
            "Epoch 135/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 0.9958 - val_loss: 1.6324 - val_accuracy: 0.8750\n",
            "Epoch 136/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.4897 - accuracy: 0.9406 - val_loss: 2.1129 - val_accuracy: 0.7906\n",
            "Epoch 137/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.6025 - accuracy: 0.9103 - val_loss: 1.3894 - val_accuracy: 0.8156\n",
            "Epoch 138/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.2180 - accuracy: 0.9510 - val_loss: 1.1949 - val_accuracy: 0.8500\n",
            "Epoch 139/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1080 - accuracy: 0.9729 - val_loss: 1.2588 - val_accuracy: 0.8625\n",
            "Epoch 140/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0586 - accuracy: 0.9854 - val_loss: 1.3084 - val_accuracy: 0.8531\n",
            "Epoch 141/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.9937 - val_loss: 1.3382 - val_accuracy: 0.8687\n",
            "Epoch 142/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0301 - accuracy: 0.9948 - val_loss: 1.3709 - val_accuracy: 0.8656\n",
            "Epoch 143/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.9958 - val_loss: 1.4127 - val_accuracy: 0.8719\n",
            "Epoch 144/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.9979 - val_loss: 1.4638 - val_accuracy: 0.8625\n",
            "Epoch 145/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.9979 - val_loss: 1.4373 - val_accuracy: 0.8687\n",
            "Epoch 146/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0158 - accuracy: 0.9979 - val_loss: 1.4496 - val_accuracy: 0.8687\n",
            "Epoch 147/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0144 - accuracy: 0.9979 - val_loss: 1.4800 - val_accuracy: 0.8687\n",
            "Epoch 148/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 0.9979 - val_loss: 1.4910 - val_accuracy: 0.8719\n",
            "Epoch 149/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0117 - accuracy: 0.9990 - val_loss: 1.5422 - val_accuracy: 0.8687\n",
            "Epoch 150/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9990 - val_loss: 1.5240 - val_accuracy: 0.8656\n",
            "Epoch 151/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.9990 - val_loss: 1.5523 - val_accuracy: 0.8719\n",
            "Epoch 152/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.9990 - val_loss: 1.5384 - val_accuracy: 0.8687\n",
            "Epoch 153/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.9990 - val_loss: 1.5632 - val_accuracy: 0.8719\n",
            "Epoch 154/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.9990 - val_loss: 1.5629 - val_accuracy: 0.8687\n",
            "Epoch 155/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9990 - val_loss: 1.5935 - val_accuracy: 0.8719\n",
            "Epoch 156/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9990 - val_loss: 1.5904 - val_accuracy: 0.8719\n",
            "Epoch 157/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9990 - val_loss: 1.6308 - val_accuracy: 0.8750\n",
            "Epoch 158/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9990 - val_loss: 1.5690 - val_accuracy: 0.8687\n",
            "Epoch 159/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9979 - val_loss: 1.6446 - val_accuracy: 0.8719\n",
            "Epoch 160/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9990 - val_loss: 1.5975 - val_accuracy: 0.8781\n",
            "Epoch 161/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9979 - val_loss: 1.6426 - val_accuracy: 0.8719\n",
            "Epoch 162/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.9979 - val_loss: 1.6517 - val_accuracy: 0.8750\n",
            "Epoch 163/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0068 - accuracy: 0.9990 - val_loss: 1.6477 - val_accuracy: 0.8750\n",
            "Epoch 164/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0059 - accuracy: 0.9990 - val_loss: 1.6769 - val_accuracy: 0.8750\n",
            "Epoch 165/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0053 - accuracy: 0.9979 - val_loss: 1.7161 - val_accuracy: 0.8750\n",
            "Epoch 166/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9990 - val_loss: 1.7408 - val_accuracy: 0.8750\n",
            "Epoch 167/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.9990 - val_loss: 1.6542 - val_accuracy: 0.8750\n",
            "Epoch 168/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 1.6774 - val_accuracy: 0.8750\n",
            "Epoch 169/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.9979 - val_loss: 1.7488 - val_accuracy: 0.8750\n",
            "Epoch 170/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 1.7102 - val_accuracy: 0.8687\n",
            "Epoch 171/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 1.6935 - val_accuracy: 0.8750\n",
            "Epoch 172/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 1.7268 - val_accuracy: 0.8750\n",
            "Epoch 173/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 1.7392 - val_accuracy: 0.8750\n",
            "Epoch 174/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 1.7417 - val_accuracy: 0.8750\n",
            "Epoch 175/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 1.7782 - val_accuracy: 0.8750\n",
            "Epoch 176/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 1.7481 - val_accuracy: 0.8781\n",
            "Epoch 177/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.7871 - val_accuracy: 0.8750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc2002159b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9kSlesN1uW6"
      },
      "source": [
        "### Visualising the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ilHD45N1v7r"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrmVbS9S3TGe"
      },
      "source": [
        "# Predict\n",
        "y_pred2 = weighted_classifier.predict(X_test_scaled) > 0.5"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEZjloMk3lBT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "7d4cc5e0-1f25-4b17-9ef8-9714e171fde2"
      },
      "source": [
        "# Classification Report for Baseline NN Classifier with Weighted Classes\n",
        "pd.DataFrame(classification_report(y_test, y_pred2, output_dict=True))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>weighted avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.942652</td>\n",
              "      <td>0.512195</td>\n",
              "      <td>0.8875</td>\n",
              "      <td>0.727424</td>\n",
              "      <td>0.892881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.929329</td>\n",
              "      <td>0.567568</td>\n",
              "      <td>0.8875</td>\n",
              "      <td>0.748448</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.935943</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.8875</td>\n",
              "      <td>0.737202</td>\n",
              "      <td>0.889984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>283.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>0.8875</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>320.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    0          1  accuracy   macro avg  weighted avg\n",
              "precision    0.942652   0.512195    0.8875    0.727424      0.892881\n",
              "recall       0.929329   0.567568    0.8875    0.748448      0.887500\n",
              "f1-score     0.935943   0.538462    0.8875    0.737202      0.889984\n",
              "support    283.000000  37.000000    0.8875  320.000000    320.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6S_ZNVPX20k"
      },
      "source": [
        "## 2.2 Hyperparameter Tuning of Baseline NN Classifier\n",
        "\n",
        "\n",
        "1.   No. of hidden units\n",
        "2.   Learning Rate\n",
        "3.   Mini-Batch size\n",
        "4.   No. of layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq_7yrGx8DKq"
      },
      "source": [
        "class tuned_classifier(HyperModel):\n",
        "  def __init__(self, input_size=13):\n",
        "    self.input_size = input_size\n",
        "  \n",
        "  def build(self, hp):\n",
        "    model = Sequential()\n",
        "    # Input and First Layer\n",
        "    model.add(Dense(units=hp.Int('units_1', min_value=32, max_value=256, step=32, default=128), input_dim=self.input_size, activation=hp.Choice('dense_activation_1', values=['relu', 'tanh'], default='relu')))\n",
        "    # Second Layer\n",
        "    model.add(Dense(units=hp.Int('units_2', min_value=32, max_value=256, step=32, default=128), activation=hp.Choice('dense_activation_2', values=['relu', 'tanh'], default='relu')))\n",
        "    # Third Layer\n",
        "    model.add(Dense(units=hp.Int('units_3', min_value=32, max_value=256, step=32, default=128), activation=hp.Choice('dense_activation_3', values=['relu', 'tanh'], default='relu')))\n",
        "    # Output Layer\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Compile Model\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)), metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYbjibaOCGzQ"
      },
      "source": [
        "# Instantiate Hyper Model\n",
        "hypermodel = tuned_classifier()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq67xWlHCNSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef7d735-3ad6-4f9b-f9ee-eb04fc279a3a"
      },
      "source": [
        "# Instantiate Tuner\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    max_epochs=10,\n",
        "    objective='val_accuracy',\n",
        "    executions_per_trial=2,\n",
        "    seed=42,\n",
        "    project_name='red_wine_classification'\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project ./red_wine_classification/oracle.json\n",
            "INFO:tensorflow:Reloading Tuner from ./red_wine_classification/tuner0.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHBdhZdtDgC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3929f160-e8c5-4abd-bd5f-ca6cca83c849"
      },
      "source": [
        "tuner.search_space_summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 7\n",
            "units_1 (Int)\n",
            "{'default': 128, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n",
            "dense_activation_1 (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
            "units_2 (Int)\n",
            "{'default': 128, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n",
            "dense_activation_2 (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
            "units_3 (Int)\n",
            "{'default': 128, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': None}\n",
            "dense_activation_3 (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh'], 'ordered': False}\n",
            "learning_rate (Float)\n",
            "{'default': 0.001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxNIYMdNDstL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c739e83c-342a-4927-e657-9c50da951bdd"
      },
      "source": [
        "# Randomised Search\n",
        "tuner.search(X_train, y_train, epochs=5, validation_split=0.1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFezISIED12a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926b64fd-1526-473e-c4a8-1a3cc32fd895"
      },
      "source": [
        "# Retrieve the best model.\n",
        "best_hps = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Show summary of Best Model\n",
        "best_hps.get_config()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'layers': [{'class_name': 'InputLayer',\n",
              "   'config': {'batch_input_shape': (None, 13),\n",
              "    'dtype': 'float32',\n",
              "    'name': 'dense_input',\n",
              "    'ragged': False,\n",
              "    'sparse': False}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'batch_input_shape': (None, 13),\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense',\n",
              "    'trainable': True,\n",
              "    'units': 128,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_1',\n",
              "    'trainable': True,\n",
              "    'units': 256,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'relu',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_2',\n",
              "    'trainable': True,\n",
              "    'units': 224,\n",
              "    'use_bias': True}},\n",
              "  {'class_name': 'Dense',\n",
              "   'config': {'activation': 'sigmoid',\n",
              "    'activity_regularizer': None,\n",
              "    'bias_constraint': None,\n",
              "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
              "    'bias_regularizer': None,\n",
              "    'dtype': 'float32',\n",
              "    'kernel_constraint': None,\n",
              "    'kernel_initializer': {'class_name': 'GlorotUniform',\n",
              "     'config': {'seed': None}},\n",
              "    'kernel_regularizer': None,\n",
              "    'name': 'dense_3',\n",
              "    'trainable': True,\n",
              "    'units': 1,\n",
              "    'use_bias': True}}],\n",
              " 'name': 'sequential'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpiXQSO6DFrf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e74c333-b4d7-4ac4-830f-c587f369ddd0"
      },
      "source": [
        "best_hps.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               1792      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 224)               57568     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 225       \n",
            "=================================================================\n",
            "Total params: 92,609\n",
            "Trainable params: 92,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA4_yMERL6MZ"
      },
      "source": [
        "# Build optimal model using best hyperparameters\n",
        "hps_best_params = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
        "optimal_classifier = tuner.hypermodel.build(hps_best_params)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCAdleNYqIj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ce3ea3-11fe-4ed4-f295-2cbdd8f540d4"
      },
      "source": [
        "# Create call backs\n",
        "my_callbacks_opt = [EarlyStopping(monitor='val_accuracy', patience=50, mode='max')]\n",
        "\n",
        "# Train model\n",
        "optimal_classifier.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), class_weight=weights, epochs=200, batch_size=16, callbacks=my_callbacks_opt)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 1.1842 - accuracy: 0.6663 - val_loss: 0.6825 - val_accuracy: 0.6406\n",
            "Epoch 2/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.8886 - accuracy: 0.7278 - val_loss: 0.5155 - val_accuracy: 0.7281\n",
            "Epoch 3/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.8231 - accuracy: 0.7518 - val_loss: 0.5600 - val_accuracy: 0.7312\n",
            "Epoch 4/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.7249 - accuracy: 0.7633 - val_loss: 0.3607 - val_accuracy: 0.8062\n",
            "Epoch 5/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.7371 - accuracy: 0.7967 - val_loss: 0.3966 - val_accuracy: 0.8000\n",
            "Epoch 6/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.6586 - accuracy: 0.8019 - val_loss: 0.5956 - val_accuracy: 0.7812\n",
            "Epoch 7/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.6308 - accuracy: 0.8040 - val_loss: 0.4719 - val_accuracy: 0.7875\n",
            "Epoch 8/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.6291 - accuracy: 0.8123 - val_loss: 0.8684 - val_accuracy: 0.6531\n",
            "Epoch 9/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.5890 - accuracy: 0.8186 - val_loss: 0.4574 - val_accuracy: 0.7750\n",
            "Epoch 10/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5046 - accuracy: 0.8457 - val_loss: 0.5139 - val_accuracy: 0.7969\n",
            "Epoch 11/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4759 - accuracy: 0.8540 - val_loss: 0.5212 - val_accuracy: 0.7781\n",
            "Epoch 12/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4149 - accuracy: 0.8843 - val_loss: 0.6201 - val_accuracy: 0.7594\n",
            "Epoch 13/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3953 - accuracy: 0.8884 - val_loss: 0.6432 - val_accuracy: 0.7906\n",
            "Epoch 14/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3624 - accuracy: 0.8999 - val_loss: 0.6585 - val_accuracy: 0.8000\n",
            "Epoch 15/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.3724 - accuracy: 0.8999 - val_loss: 0.5934 - val_accuracy: 0.8062\n",
            "Epoch 16/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.3889 - accuracy: 0.8832 - val_loss: 0.5555 - val_accuracy: 0.8188\n",
            "Epoch 17/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3592 - accuracy: 0.8989 - val_loss: 0.6065 - val_accuracy: 0.7719\n",
            "Epoch 18/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.2721 - accuracy: 0.9291 - val_loss: 0.7313 - val_accuracy: 0.7875\n",
            "Epoch 19/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.2883 - accuracy: 0.9082 - val_loss: 0.6271 - val_accuracy: 0.8344\n",
            "Epoch 20/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.2814 - accuracy: 0.9187 - val_loss: 0.6792 - val_accuracy: 0.7875\n",
            "Epoch 21/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2272 - accuracy: 0.9322 - val_loss: 0.8938 - val_accuracy: 0.7594\n",
            "Epoch 22/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2580 - accuracy: 0.9312 - val_loss: 0.9180 - val_accuracy: 0.7812\n",
            "Epoch 23/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.4115 - accuracy: 0.9197 - val_loss: 0.6064 - val_accuracy: 0.8250\n",
            "Epoch 24/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.2235 - accuracy: 0.9406 - val_loss: 0.7616 - val_accuracy: 0.8125\n",
            "Epoch 25/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1956 - accuracy: 0.9499 - val_loss: 0.6723 - val_accuracy: 0.8469\n",
            "Epoch 26/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1190 - accuracy: 0.9677 - val_loss: 0.7655 - val_accuracy: 0.8562\n",
            "Epoch 27/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1726 - accuracy: 0.9656 - val_loss: 0.5974 - val_accuracy: 0.8750\n",
            "Epoch 28/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.9583 - val_loss: 0.6231 - val_accuracy: 0.8687\n",
            "Epoch 29/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 0.9760 - val_loss: 0.6675 - val_accuracy: 0.8625\n",
            "Epoch 30/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0768 - accuracy: 0.9812 - val_loss: 0.7008 - val_accuracy: 0.8656\n",
            "Epoch 31/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9833 - val_loss: 0.7395 - val_accuracy: 0.8594\n",
            "Epoch 32/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0647 - accuracy: 0.9854 - val_loss: 0.8013 - val_accuracy: 0.8656\n",
            "Epoch 33/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9885 - val_loss: 0.8000 - val_accuracy: 0.8844\n",
            "Epoch 34/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0535 - accuracy: 0.9875 - val_loss: 0.8181 - val_accuracy: 0.8750\n",
            "Epoch 35/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0383 - accuracy: 0.9896 - val_loss: 0.8498 - val_accuracy: 0.8781\n",
            "Epoch 36/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0654 - accuracy: 0.9833 - val_loss: 0.7377 - val_accuracy: 0.8719\n",
            "Epoch 37/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9875 - val_loss: 0.7774 - val_accuracy: 0.8781\n",
            "Epoch 38/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1781 - accuracy: 0.9718 - val_loss: 0.8457 - val_accuracy: 0.8375\n",
            "Epoch 39/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.4069 - accuracy: 0.9239 - val_loss: 0.6775 - val_accuracy: 0.8438\n",
            "Epoch 40/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2912 - accuracy: 0.9197 - val_loss: 0.6174 - val_accuracy: 0.8469\n",
            "Epoch 41/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2032 - accuracy: 0.9572 - val_loss: 0.6151 - val_accuracy: 0.8687\n",
            "Epoch 42/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2030 - accuracy: 0.9562 - val_loss: 0.6689 - val_accuracy: 0.8500\n",
            "Epoch 43/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0916 - accuracy: 0.9802 - val_loss: 0.6748 - val_accuracy: 0.8469\n",
            "Epoch 44/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9885 - val_loss: 0.7021 - val_accuracy: 0.8719\n",
            "Epoch 45/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0472 - accuracy: 0.9927 - val_loss: 0.8135 - val_accuracy: 0.8719\n",
            "Epoch 46/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0463 - accuracy: 0.9917 - val_loss: 0.8014 - val_accuracy: 0.8719\n",
            "Epoch 47/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.9937 - val_loss: 0.8250 - val_accuracy: 0.8813\n",
            "Epoch 48/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.9937 - val_loss: 0.8449 - val_accuracy: 0.8781\n",
            "Epoch 49/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 0.9969 - val_loss: 0.9205 - val_accuracy: 0.8719\n",
            "Epoch 50/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.9948 - val_loss: 0.9009 - val_accuracy: 0.8844\n",
            "Epoch 51/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.9969 - val_loss: 0.8768 - val_accuracy: 0.8719\n",
            "Epoch 52/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.9969 - val_loss: 0.9633 - val_accuracy: 0.8781\n",
            "Epoch 53/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0179 - accuracy: 0.9969 - val_loss: 0.9807 - val_accuracy: 0.8750\n",
            "Epoch 54/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 0.9979 - val_loss: 0.9940 - val_accuracy: 0.8781\n",
            "Epoch 55/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 0.9979 - val_loss: 0.9929 - val_accuracy: 0.8781\n",
            "Epoch 56/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.9979 - val_loss: 0.9920 - val_accuracy: 0.8813\n",
            "Epoch 57/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.9979 - val_loss: 0.9655 - val_accuracy: 0.8813\n",
            "Epoch 58/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.9979 - val_loss: 1.0239 - val_accuracy: 0.8844\n",
            "Epoch 59/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9979 - val_loss: 1.0290 - val_accuracy: 0.8844\n",
            "Epoch 60/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.9979 - val_loss: 1.0381 - val_accuracy: 0.8844\n",
            "Epoch 61/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.9979 - val_loss: 1.0180 - val_accuracy: 0.8781\n",
            "Epoch 62/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9979 - val_loss: 1.0450 - val_accuracy: 0.8813\n",
            "Epoch 63/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9979 - val_loss: 1.0682 - val_accuracy: 0.8844\n",
            "Epoch 64/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.9979 - val_loss: 1.0866 - val_accuracy: 0.8781\n",
            "Epoch 65/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9979 - val_loss: 1.1151 - val_accuracy: 0.8813\n",
            "Epoch 66/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0094 - accuracy: 0.9979 - val_loss: 1.0923 - val_accuracy: 0.8813\n",
            "Epoch 67/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.9979 - val_loss: 1.0864 - val_accuracy: 0.8844\n",
            "Epoch 68/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9979 - val_loss: 1.0814 - val_accuracy: 0.8844\n",
            "Epoch 69/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.9979 - val_loss: 1.0950 - val_accuracy: 0.8813\n",
            "Epoch 70/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0094 - accuracy: 0.9979 - val_loss: 1.1156 - val_accuracy: 0.8813\n",
            "Epoch 71/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9979 - val_loss: 1.1149 - val_accuracy: 0.8719\n",
            "Epoch 72/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9979 - val_loss: 1.1138 - val_accuracy: 0.8813\n",
            "Epoch 73/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 1.1593 - val_accuracy: 0.8813\n",
            "Epoch 74/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0124 - accuracy: 0.9979 - val_loss: 1.1411 - val_accuracy: 0.8813\n",
            "Epoch 75/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.9979 - val_loss: 1.1818 - val_accuracy: 0.8813\n",
            "Epoch 76/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9979 - val_loss: 1.1861 - val_accuracy: 0.8813\n",
            "Epoch 77/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.9937 - val_loss: 1.5265 - val_accuracy: 0.8438\n",
            "Epoch 78/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.9258 - accuracy: 0.8384 - val_loss: 0.4745 - val_accuracy: 0.7875\n",
            "Epoch 79/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4585 - accuracy: 0.8457 - val_loss: 0.6712 - val_accuracy: 0.7969\n",
            "Epoch 80/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2911 - accuracy: 0.9281 - val_loss: 0.5976 - val_accuracy: 0.8188\n",
            "Epoch 81/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1893 - accuracy: 0.9468 - val_loss: 0.7411 - val_accuracy: 0.8500\n",
            "Epoch 82/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1422 - accuracy: 0.9604 - val_loss: 0.8607 - val_accuracy: 0.8500\n",
            "Epoch 83/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.1889 - accuracy: 0.9614 - val_loss: 0.6031 - val_accuracy: 0.8781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc20015f470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFAWX7HohUdg"
      },
      "source": [
        "# Optimal model predictions\n",
        "y_pred3 = optimal_classifier.predict(X_test_scaled) > 0.5"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM8Cdd-OF2-m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "588a1d7d-6b25-4ead-f444-1578a2a347b8"
      },
      "source": [
        "# Classification Report of Tuned Baseline NN Classifier\n",
        "pd.DataFrame(classification_report(y_test, y_pred3, output_dict=True))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>weighted avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.958491</td>\n",
              "      <td>0.472727</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.715609</td>\n",
              "      <td>0.902324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.897527</td>\n",
              "      <td>0.702703</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.800115</td>\n",
              "      <td>0.875000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.927007</td>\n",
              "      <td>0.565217</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.746112</td>\n",
              "      <td>0.885175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>283.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>320.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    0          1  accuracy   macro avg  weighted avg\n",
              "precision    0.958491   0.472727     0.875    0.715609      0.902324\n",
              "recall       0.897527   0.702703     0.875    0.800115      0.875000\n",
              "f1-score     0.927007   0.565217     0.875    0.746112      0.885175\n",
              "support    283.000000  37.000000     0.875  320.000000    320.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPlML8zMXiCc"
      },
      "source": [
        "## 2.3 Tuned NN Classifier with Dropout Regularisation & Batch Normalisation\n",
        "### 2.3.1 with Dropout\n",
        "We will set keep_prob = 0.8 or dropout_rate = 0.2 as a baseline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS9eeWl82gQS"
      },
      "source": [
        "def build_dropout_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, input_dim=13, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(224, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68IBfh0ebVXa"
      },
      "source": [
        "# Create Pipeline for Dropout\n",
        "pipe_dropout = []\n",
        "pipe_dropout.append(('standardize', StandardScaler()))\n",
        "pipe_dropout.append(('mlp', KerasClassifier(build_fn=build_dropout_model, epochs=100, batch_size=16, verbose=0)))\n",
        "pipeline_dropout = Pipeline(pipe_dropout)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flmj4xLdbY_7"
      },
      "source": [
        "# Train and Predict\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "results_dropout = cross_val_score(pipeline_dropout, final_df, target, cv=kfold)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ7l4M1NbasS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a5a6f1-ab50-460f-e73c-5e26d7c53884"
      },
      "source": [
        "print(\"Baseline NN Classifier with Dropout has a mean cross val accuracy of {:.2f}%\".format(results_dropout.mean()*100))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline NN Classifier with Dropout has a mean cross val accuracy of 89.31%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtKehn4YkDEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85478ecf-904f-4acf-9895-1448f2c19166"
      },
      "source": [
        "# Dropout model & F1\n",
        "dropout_model = build_dropout_model()\n",
        "\n",
        "# Create call backs\n",
        "my_callbacks_dropout = [EarlyStopping(monitor='val_accuracy', patience=50, mode='max')]\n",
        "\n",
        "# Train model\n",
        "dropout_model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), class_weight=weights, epochs=200, batch_size=16, callbacks=my_callbacks_dropout)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 1.1192 - accuracy: 0.7414 - val_loss: 0.6370 - val_accuracy: 0.6844\n",
            "Epoch 2/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.9196 - accuracy: 0.7258 - val_loss: 0.5930 - val_accuracy: 0.6906\n",
            "Epoch 3/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.8765 - accuracy: 0.7456 - val_loss: 0.5904 - val_accuracy: 0.6906\n",
            "Epoch 4/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.8376 - accuracy: 0.7351 - val_loss: 0.6638 - val_accuracy: 0.7000\n",
            "Epoch 5/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.8378 - accuracy: 0.7466 - val_loss: 0.5872 - val_accuracy: 0.7000\n",
            "Epoch 6/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.7651 - accuracy: 0.7508 - val_loss: 0.4837 - val_accuracy: 0.7406\n",
            "Epoch 7/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.7722 - accuracy: 0.7487 - val_loss: 0.4878 - val_accuracy: 0.7500\n",
            "Epoch 8/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.7451 - accuracy: 0.7643 - val_loss: 0.4393 - val_accuracy: 0.7656\n",
            "Epoch 9/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.7438 - accuracy: 0.7696 - val_loss: 0.4492 - val_accuracy: 0.7781\n",
            "Epoch 10/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.7430 - accuracy: 0.7539 - val_loss: 0.5280 - val_accuracy: 0.7344\n",
            "Epoch 11/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.6714 - accuracy: 0.7758 - val_loss: 0.5618 - val_accuracy: 0.7281\n",
            "Epoch 12/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.6708 - accuracy: 0.7810 - val_loss: 0.5353 - val_accuracy: 0.7500\n",
            "Epoch 13/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.6896 - accuracy: 0.7946 - val_loss: 0.5730 - val_accuracy: 0.7063\n",
            "Epoch 14/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.6422 - accuracy: 0.8029 - val_loss: 0.5415 - val_accuracy: 0.7219\n",
            "Epoch 15/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.6261 - accuracy: 0.8123 - val_loss: 0.7351 - val_accuracy: 0.7031\n",
            "Epoch 16/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.6772 - accuracy: 0.7862 - val_loss: 0.5108 - val_accuracy: 0.7250\n",
            "Epoch 17/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5888 - accuracy: 0.8248 - val_loss: 0.6180 - val_accuracy: 0.7250\n",
            "Epoch 18/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5769 - accuracy: 0.8175 - val_loss: 0.5687 - val_accuracy: 0.7469\n",
            "Epoch 19/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5718 - accuracy: 0.8206 - val_loss: 0.6109 - val_accuracy: 0.7188\n",
            "Epoch 20/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5639 - accuracy: 0.8217 - val_loss: 0.5649 - val_accuracy: 0.7344\n",
            "Epoch 21/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5190 - accuracy: 0.8311 - val_loss: 0.5299 - val_accuracy: 0.7875\n",
            "Epoch 22/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5760 - accuracy: 0.8269 - val_loss: 0.5641 - val_accuracy: 0.7312\n",
            "Epoch 23/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4627 - accuracy: 0.8644 - val_loss: 0.6512 - val_accuracy: 0.7344\n",
            "Epoch 24/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5618 - accuracy: 0.8206 - val_loss: 0.6045 - val_accuracy: 0.7688\n",
            "Epoch 25/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5913 - accuracy: 0.8394 - val_loss: 0.6180 - val_accuracy: 0.7219\n",
            "Epoch 26/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5464 - accuracy: 0.8279 - val_loss: 0.5720 - val_accuracy: 0.7594\n",
            "Epoch 27/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5200 - accuracy: 0.8123 - val_loss: 0.5220 - val_accuracy: 0.7656\n",
            "Epoch 28/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4753 - accuracy: 0.8519 - val_loss: 0.6038 - val_accuracy: 0.7500\n",
            "Epoch 29/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.5093 - accuracy: 0.8655 - val_loss: 0.6317 - val_accuracy: 0.7531\n",
            "Epoch 30/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4605 - accuracy: 0.8488 - val_loss: 0.6043 - val_accuracy: 0.7656\n",
            "Epoch 31/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4559 - accuracy: 0.8582 - val_loss: 0.6091 - val_accuracy: 0.7719\n",
            "Epoch 32/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3856 - accuracy: 0.8895 - val_loss: 0.6196 - val_accuracy: 0.8062\n",
            "Epoch 33/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3946 - accuracy: 0.8947 - val_loss: 0.7190 - val_accuracy: 0.7906\n",
            "Epoch 34/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4597 - accuracy: 0.8603 - val_loss: 0.6526 - val_accuracy: 0.7625\n",
            "Epoch 35/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3656 - accuracy: 0.8811 - val_loss: 0.6092 - val_accuracy: 0.8156\n",
            "Epoch 36/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4299 - accuracy: 0.8801 - val_loss: 0.7147 - val_accuracy: 0.7406\n",
            "Epoch 37/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.4553 - accuracy: 0.8655 - val_loss: 0.6347 - val_accuracy: 0.7750\n",
            "Epoch 38/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3417 - accuracy: 0.8822 - val_loss: 0.5884 - val_accuracy: 0.8094\n",
            "Epoch 39/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3090 - accuracy: 0.9030 - val_loss: 0.6647 - val_accuracy: 0.8313\n",
            "Epoch 40/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3511 - accuracy: 0.9062 - val_loss: 0.6580 - val_accuracy: 0.8344\n",
            "Epoch 41/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3156 - accuracy: 0.9041 - val_loss: 0.5968 - val_accuracy: 0.8188\n",
            "Epoch 42/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3645 - accuracy: 0.8968 - val_loss: 0.7306 - val_accuracy: 0.8156\n",
            "Epoch 43/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3615 - accuracy: 0.8926 - val_loss: 0.6462 - val_accuracy: 0.7719\n",
            "Epoch 44/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3156 - accuracy: 0.9041 - val_loss: 0.6078 - val_accuracy: 0.8469\n",
            "Epoch 45/200\n",
            "60/60 [==============================] - 0s 3ms/step - loss: 0.2842 - accuracy: 0.9103 - val_loss: 0.7082 - val_accuracy: 0.8000\n",
            "Epoch 46/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2949 - accuracy: 0.9228 - val_loss: 0.7715 - val_accuracy: 0.8219\n",
            "Epoch 47/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2677 - accuracy: 0.9249 - val_loss: 0.7127 - val_accuracy: 0.8344\n",
            "Epoch 48/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3387 - accuracy: 0.9030 - val_loss: 0.6945 - val_accuracy: 0.8375\n",
            "Epoch 49/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3033 - accuracy: 0.9145 - val_loss: 0.7374 - val_accuracy: 0.8031\n",
            "Epoch 50/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2868 - accuracy: 0.9124 - val_loss: 0.6986 - val_accuracy: 0.8313\n",
            "Epoch 51/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2153 - accuracy: 0.9364 - val_loss: 0.7160 - val_accuracy: 0.8469\n",
            "Epoch 52/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2443 - accuracy: 0.9312 - val_loss: 0.7106 - val_accuracy: 0.8562\n",
            "Epoch 53/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3080 - accuracy: 0.9166 - val_loss: 0.7001 - val_accuracy: 0.8313\n",
            "Epoch 54/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2588 - accuracy: 0.9270 - val_loss: 0.7395 - val_accuracy: 0.8062\n",
            "Epoch 55/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2559 - accuracy: 0.9218 - val_loss: 0.7615 - val_accuracy: 0.8219\n",
            "Epoch 56/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2353 - accuracy: 0.9333 - val_loss: 0.6970 - val_accuracy: 0.8375\n",
            "Epoch 57/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2020 - accuracy: 0.9416 - val_loss: 0.7392 - val_accuracy: 0.8625\n",
            "Epoch 58/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2083 - accuracy: 0.9426 - val_loss: 0.7877 - val_accuracy: 0.8250\n",
            "Epoch 59/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2454 - accuracy: 0.9416 - val_loss: 0.7538 - val_accuracy: 0.8531\n",
            "Epoch 60/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2432 - accuracy: 0.9426 - val_loss: 0.7637 - val_accuracy: 0.8500\n",
            "Epoch 61/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.3059 - accuracy: 0.9218 - val_loss: 0.6873 - val_accuracy: 0.8125\n",
            "Epoch 62/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2172 - accuracy: 0.9353 - val_loss: 0.7580 - val_accuracy: 0.8156\n",
            "Epoch 63/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2281 - accuracy: 0.9260 - val_loss: 0.7168 - val_accuracy: 0.8375\n",
            "Epoch 64/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2845 - accuracy: 0.9281 - val_loss: 0.6573 - val_accuracy: 0.8250\n",
            "Epoch 65/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2811 - accuracy: 0.9218 - val_loss: 0.6992 - val_accuracy: 0.8313\n",
            "Epoch 66/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2240 - accuracy: 0.9406 - val_loss: 0.7339 - val_accuracy: 0.8469\n",
            "Epoch 67/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1866 - accuracy: 0.9468 - val_loss: 0.6714 - val_accuracy: 0.8375\n",
            "Epoch 68/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1818 - accuracy: 0.9479 - val_loss: 0.6803 - val_accuracy: 0.8562\n",
            "Epoch 69/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1998 - accuracy: 0.9489 - val_loss: 0.6975 - val_accuracy: 0.8625\n",
            "Epoch 70/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2250 - accuracy: 0.9333 - val_loss: 0.7477 - val_accuracy: 0.8344\n",
            "Epoch 71/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1987 - accuracy: 0.9499 - val_loss: 0.7782 - val_accuracy: 0.8156\n",
            "Epoch 72/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2230 - accuracy: 0.9406 - val_loss: 0.7017 - val_accuracy: 0.8531\n",
            "Epoch 73/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1926 - accuracy: 0.9499 - val_loss: 0.7807 - val_accuracy: 0.8406\n",
            "Epoch 74/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2212 - accuracy: 0.9489 - val_loss: 0.6317 - val_accuracy: 0.8531\n",
            "Epoch 75/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2096 - accuracy: 0.9458 - val_loss: 0.6087 - val_accuracy: 0.8719\n",
            "Epoch 76/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1906 - accuracy: 0.9541 - val_loss: 0.7361 - val_accuracy: 0.8687\n",
            "Epoch 77/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1741 - accuracy: 0.9489 - val_loss: 0.7131 - val_accuracy: 0.8625\n",
            "Epoch 78/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1897 - accuracy: 0.9520 - val_loss: 0.7042 - val_accuracy: 0.8500\n",
            "Epoch 79/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1773 - accuracy: 0.9499 - val_loss: 0.7087 - val_accuracy: 0.8781\n",
            "Epoch 80/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1301 - accuracy: 0.9625 - val_loss: 0.7758 - val_accuracy: 0.8719\n",
            "Epoch 81/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1388 - accuracy: 0.9666 - val_loss: 0.8513 - val_accuracy: 0.8594\n",
            "Epoch 82/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1600 - accuracy: 0.9593 - val_loss: 0.7406 - val_accuracy: 0.8625\n",
            "Epoch 83/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1229 - accuracy: 0.9645 - val_loss: 0.8260 - val_accuracy: 0.8813\n",
            "Epoch 84/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2031 - accuracy: 0.9552 - val_loss: 0.8359 - val_accuracy: 0.8438\n",
            "Epoch 85/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1788 - accuracy: 0.9520 - val_loss: 0.8925 - val_accuracy: 0.8562\n",
            "Epoch 86/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1854 - accuracy: 0.9552 - val_loss: 0.9026 - val_accuracy: 0.8406\n",
            "Epoch 87/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1894 - accuracy: 0.9447 - val_loss: 0.8139 - val_accuracy: 0.8687\n",
            "Epoch 88/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2130 - accuracy: 0.9583 - val_loss: 0.7342 - val_accuracy: 0.8438\n",
            "Epoch 89/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1358 - accuracy: 0.9604 - val_loss: 0.6990 - val_accuracy: 0.8687\n",
            "Epoch 90/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1804 - accuracy: 0.9593 - val_loss: 0.6706 - val_accuracy: 0.8531\n",
            "Epoch 91/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2049 - accuracy: 0.9562 - val_loss: 0.7508 - val_accuracy: 0.8313\n",
            "Epoch 92/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1407 - accuracy: 0.9687 - val_loss: 0.7238 - val_accuracy: 0.8531\n",
            "Epoch 93/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1429 - accuracy: 0.9635 - val_loss: 0.7409 - val_accuracy: 0.8625\n",
            "Epoch 94/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1448 - accuracy: 0.9625 - val_loss: 0.7340 - val_accuracy: 0.8625\n",
            "Epoch 95/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1812 - accuracy: 0.9583 - val_loss: 0.7092 - val_accuracy: 0.8313\n",
            "Epoch 96/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1798 - accuracy: 0.9510 - val_loss: 0.7507 - val_accuracy: 0.8594\n",
            "Epoch 97/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1040 - accuracy: 0.9729 - val_loss: 0.7896 - val_accuracy: 0.8719\n",
            "Epoch 98/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1498 - accuracy: 0.9666 - val_loss: 0.7874 - val_accuracy: 0.8719\n",
            "Epoch 99/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1132 - accuracy: 0.9698 - val_loss: 0.8561 - val_accuracy: 0.8531\n",
            "Epoch 100/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9750 - val_loss: 0.8070 - val_accuracy: 0.8500\n",
            "Epoch 101/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1231 - accuracy: 0.9739 - val_loss: 0.8175 - val_accuracy: 0.8750\n",
            "Epoch 102/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2164 - accuracy: 0.9562 - val_loss: 0.7867 - val_accuracy: 0.8562\n",
            "Epoch 103/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2553 - accuracy: 0.9604 - val_loss: 0.7798 - val_accuracy: 0.8406\n",
            "Epoch 104/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2751 - accuracy: 0.9447 - val_loss: 0.6520 - val_accuracy: 0.8531\n",
            "Epoch 105/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1447 - accuracy: 0.9593 - val_loss: 0.6970 - val_accuracy: 0.8719\n",
            "Epoch 106/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1728 - accuracy: 0.9489 - val_loss: 0.6734 - val_accuracy: 0.8656\n",
            "Epoch 107/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1099 - accuracy: 0.9645 - val_loss: 0.7165 - val_accuracy: 0.8687\n",
            "Epoch 108/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2598 - accuracy: 0.9510 - val_loss: 0.6874 - val_accuracy: 0.8469\n",
            "Epoch 109/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.9718 - val_loss: 0.6693 - val_accuracy: 0.8594\n",
            "Epoch 110/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1665 - accuracy: 0.9718 - val_loss: 0.6736 - val_accuracy: 0.8656\n",
            "Epoch 111/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1582 - accuracy: 0.9604 - val_loss: 0.6701 - val_accuracy: 0.8438\n",
            "Epoch 112/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1319 - accuracy: 0.9656 - val_loss: 0.6671 - val_accuracy: 0.8719\n",
            "Epoch 113/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1556 - accuracy: 0.9614 - val_loss: 0.6595 - val_accuracy: 0.8750\n",
            "Epoch 114/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1422 - accuracy: 0.9645 - val_loss: 0.6429 - val_accuracy: 0.8719\n",
            "Epoch 115/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1152 - accuracy: 0.9729 - val_loss: 0.6701 - val_accuracy: 0.8781\n",
            "Epoch 116/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1485 - accuracy: 0.9614 - val_loss: 0.7609 - val_accuracy: 0.8500\n",
            "Epoch 117/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0982 - accuracy: 0.9791 - val_loss: 0.7042 - val_accuracy: 0.8750\n",
            "Epoch 118/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1168 - accuracy: 0.9771 - val_loss: 0.7268 - val_accuracy: 0.8813\n",
            "Epoch 119/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1408 - accuracy: 0.9635 - val_loss: 0.7330 - val_accuracy: 0.8813\n",
            "Epoch 120/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0934 - accuracy: 0.9823 - val_loss: 0.7575 - val_accuracy: 0.8781\n",
            "Epoch 121/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0699 - accuracy: 0.9864 - val_loss: 0.7665 - val_accuracy: 0.8844\n",
            "Epoch 122/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0711 - accuracy: 0.9812 - val_loss: 0.8137 - val_accuracy: 0.8750\n",
            "Epoch 123/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1183 - accuracy: 0.9812 - val_loss: 0.7685 - val_accuracy: 0.8656\n",
            "Epoch 124/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0701 - accuracy: 0.9854 - val_loss: 0.7734 - val_accuracy: 0.8750\n",
            "Epoch 125/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1181 - accuracy: 0.9750 - val_loss: 0.9178 - val_accuracy: 0.8406\n",
            "Epoch 126/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1100 - accuracy: 0.9760 - val_loss: 0.7615 - val_accuracy: 0.8750\n",
            "Epoch 127/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1110 - accuracy: 0.9760 - val_loss: 0.8482 - val_accuracy: 0.8687\n",
            "Epoch 128/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.9708 - val_loss: 0.8011 - val_accuracy: 0.8750\n",
            "Epoch 129/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1518 - accuracy: 0.9791 - val_loss: 0.7206 - val_accuracy: 0.8875\n",
            "Epoch 130/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9625 - val_loss: 0.7566 - val_accuracy: 0.8844\n",
            "Epoch 131/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0987 - accuracy: 0.9791 - val_loss: 0.7658 - val_accuracy: 0.8625\n",
            "Epoch 132/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1396 - accuracy: 0.9698 - val_loss: 0.6883 - val_accuracy: 0.8781\n",
            "Epoch 133/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0723 - accuracy: 0.9791 - val_loss: 0.7476 - val_accuracy: 0.8844\n",
            "Epoch 134/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.9802 - val_loss: 0.7739 - val_accuracy: 0.8813\n",
            "Epoch 135/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0898 - accuracy: 0.9771 - val_loss: 0.7690 - val_accuracy: 0.8938\n",
            "Epoch 136/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1276 - accuracy: 0.9708 - val_loss: 0.8661 - val_accuracy: 0.8906\n",
            "Epoch 137/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0975 - accuracy: 0.9771 - val_loss: 0.8973 - val_accuracy: 0.8687\n",
            "Epoch 138/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1392 - accuracy: 0.9708 - val_loss: 0.7825 - val_accuracy: 0.8687\n",
            "Epoch 139/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1156 - accuracy: 0.9656 - val_loss: 0.8126 - val_accuracy: 0.8813\n",
            "Epoch 140/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1705 - accuracy: 0.9739 - val_loss: 0.8095 - val_accuracy: 0.8625\n",
            "Epoch 141/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.2462 - accuracy: 0.9677 - val_loss: 0.7117 - val_accuracy: 0.8125\n",
            "Epoch 142/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.9520 - val_loss: 0.5930 - val_accuracy: 0.8625\n",
            "Epoch 143/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1410 - accuracy: 0.9645 - val_loss: 0.5995 - val_accuracy: 0.8781\n",
            "Epoch 144/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1038 - accuracy: 0.9750 - val_loss: 0.6791 - val_accuracy: 0.8750\n",
            "Epoch 145/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9802 - val_loss: 0.7356 - val_accuracy: 0.8562\n",
            "Epoch 146/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.9812 - val_loss: 0.6854 - val_accuracy: 0.8781\n",
            "Epoch 147/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0813 - accuracy: 0.9791 - val_loss: 0.6866 - val_accuracy: 0.8906\n",
            "Epoch 148/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0546 - accuracy: 0.9875 - val_loss: 0.7110 - val_accuracy: 0.9031\n",
            "Epoch 149/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0501 - accuracy: 0.9896 - val_loss: 0.7794 - val_accuracy: 0.8969\n",
            "Epoch 150/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0492 - accuracy: 0.9906 - val_loss: 0.8025 - val_accuracy: 0.8813\n",
            "Epoch 151/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.9854 - val_loss: 0.8064 - val_accuracy: 0.8781\n",
            "Epoch 152/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0468 - accuracy: 0.9875 - val_loss: 0.7772 - val_accuracy: 0.8844\n",
            "Epoch 153/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0490 - accuracy: 0.9906 - val_loss: 0.8191 - val_accuracy: 0.8906\n",
            "Epoch 154/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0550 - accuracy: 0.9885 - val_loss: 0.8314 - val_accuracy: 0.8969\n",
            "Epoch 155/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0957 - accuracy: 0.9791 - val_loss: 0.7444 - val_accuracy: 0.8750\n",
            "Epoch 156/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.9781 - val_loss: 0.7596 - val_accuracy: 0.8687\n",
            "Epoch 157/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1517 - accuracy: 0.9750 - val_loss: 0.8108 - val_accuracy: 0.8625\n",
            "Epoch 158/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0689 - accuracy: 0.9781 - val_loss: 0.8667 - val_accuracy: 0.9125\n",
            "Epoch 159/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.9885 - val_loss: 0.8700 - val_accuracy: 0.8938\n",
            "Epoch 160/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.9885 - val_loss: 0.9488 - val_accuracy: 0.8938\n",
            "Epoch 161/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0628 - accuracy: 0.9812 - val_loss: 0.9068 - val_accuracy: 0.8906\n",
            "Epoch 162/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0933 - accuracy: 0.9781 - val_loss: 0.9457 - val_accuracy: 0.8844\n",
            "Epoch 163/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1334 - accuracy: 0.9739 - val_loss: 0.8539 - val_accuracy: 0.8594\n",
            "Epoch 164/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0892 - accuracy: 0.9771 - val_loss: 0.8347 - val_accuracy: 0.8781\n",
            "Epoch 165/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.9844 - val_loss: 0.8462 - val_accuracy: 0.8938\n",
            "Epoch 166/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0477 - accuracy: 0.9906 - val_loss: 0.9014 - val_accuracy: 0.8875\n",
            "Epoch 167/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0528 - accuracy: 0.9875 - val_loss: 0.9013 - val_accuracy: 0.8844\n",
            "Epoch 168/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0495 - accuracy: 0.9875 - val_loss: 0.8885 - val_accuracy: 0.8938\n",
            "Epoch 169/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9844 - val_loss: 0.8882 - val_accuracy: 0.8906\n",
            "Epoch 170/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.9885 - val_loss: 1.0174 - val_accuracy: 0.8875\n",
            "Epoch 171/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.9864 - val_loss: 0.9920 - val_accuracy: 0.8875\n",
            "Epoch 172/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9833 - val_loss: 0.8638 - val_accuracy: 0.8719\n",
            "Epoch 173/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0923 - accuracy: 0.9812 - val_loss: 0.9023 - val_accuracy: 0.8687\n",
            "Epoch 174/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0692 - accuracy: 0.9833 - val_loss: 0.9280 - val_accuracy: 0.8781\n",
            "Epoch 175/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1392 - accuracy: 0.9791 - val_loss: 0.9345 - val_accuracy: 0.8656\n",
            "Epoch 176/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0967 - accuracy: 0.9760 - val_loss: 1.0280 - val_accuracy: 0.8594\n",
            "Epoch 177/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0759 - accuracy: 0.9823 - val_loss: 0.9083 - val_accuracy: 0.8687\n",
            "Epoch 178/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0891 - accuracy: 0.9844 - val_loss: 0.8872 - val_accuracy: 0.8844\n",
            "Epoch 179/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.9864 - val_loss: 0.9177 - val_accuracy: 0.8875\n",
            "Epoch 180/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0547 - accuracy: 0.9885 - val_loss: 0.8953 - val_accuracy: 0.8938\n",
            "Epoch 181/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0851 - accuracy: 0.9875 - val_loss: 0.9465 - val_accuracy: 0.8719\n",
            "Epoch 182/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1092 - accuracy: 0.9760 - val_loss: 1.0649 - val_accuracy: 0.8844\n",
            "Epoch 183/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.9854 - val_loss: 0.9960 - val_accuracy: 0.8875\n",
            "Epoch 184/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1237 - accuracy: 0.9739 - val_loss: 0.8498 - val_accuracy: 0.8687\n",
            "Epoch 185/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0449 - accuracy: 0.9906 - val_loss: 0.8928 - val_accuracy: 0.8750\n",
            "Epoch 186/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0563 - accuracy: 0.9864 - val_loss: 0.9403 - val_accuracy: 0.8813\n",
            "Epoch 187/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9917 - val_loss: 0.9147 - val_accuracy: 0.8938\n",
            "Epoch 188/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0359 - accuracy: 0.9937 - val_loss: 0.8991 - val_accuracy: 0.8875\n",
            "Epoch 189/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0843 - accuracy: 0.9812 - val_loss: 0.8739 - val_accuracy: 0.8656\n",
            "Epoch 190/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0614 - accuracy: 0.9812 - val_loss: 0.9162 - val_accuracy: 0.8719\n",
            "Epoch 191/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1885 - accuracy: 0.9823 - val_loss: 0.8659 - val_accuracy: 0.8875\n",
            "Epoch 192/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0330 - accuracy: 0.9917 - val_loss: 0.9245 - val_accuracy: 0.8906\n",
            "Epoch 193/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0972 - accuracy: 0.9864 - val_loss: 1.1044 - val_accuracy: 0.8656\n",
            "Epoch 194/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 0.9833 - val_loss: 0.8089 - val_accuracy: 0.8906\n",
            "Epoch 195/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0933 - accuracy: 0.9864 - val_loss: 0.8600 - val_accuracy: 0.8844\n",
            "Epoch 196/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0741 - accuracy: 0.9802 - val_loss: 0.7753 - val_accuracy: 0.8938\n",
            "Epoch 197/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.1282 - accuracy: 0.9802 - val_loss: 0.8970 - val_accuracy: 0.8813\n",
            "Epoch 198/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0914 - accuracy: 0.9791 - val_loss: 0.8945 - val_accuracy: 0.8625\n",
            "Epoch 199/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0915 - accuracy: 0.9812 - val_loss: 0.8506 - val_accuracy: 0.8844\n",
            "Epoch 200/200\n",
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0563 - accuracy: 0.9833 - val_loss: 0.8414 - val_accuracy: 0.8906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc1a79433c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwyE-uXlkpVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "ec60bc74-24b3-4c3c-dff1-0e2a8e5ad4bd"
      },
      "source": [
        "# Get predictions and F1 score\n",
        "y_pred4=dropout_model.predict(X_test_scaled) > 0.5\n",
        "\n",
        "# F1 score\n",
        "pd.DataFrame(classification_report(y_test, y_pred4, output_dict=True))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>weighted avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.942238</td>\n",
              "      <td>0.488372</td>\n",
              "      <td>0.88125</td>\n",
              "      <td>0.715305</td>\n",
              "      <td>0.889760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.922261</td>\n",
              "      <td>0.567568</td>\n",
              "      <td>0.88125</td>\n",
              "      <td>0.744915</td>\n",
              "      <td>0.881250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.932143</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>0.88125</td>\n",
              "      <td>0.728571</td>\n",
              "      <td>0.885067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>283.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>0.88125</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>320.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    0          1  accuracy   macro avg  weighted avg\n",
              "precision    0.942238   0.488372   0.88125    0.715305      0.889760\n",
              "recall       0.922261   0.567568   0.88125    0.744915      0.881250\n",
              "f1-score     0.932143   0.525000   0.88125    0.728571      0.885067\n",
              "support    283.000000  37.000000   0.88125  320.000000    320.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNkPilczblYh"
      },
      "source": [
        "#### 2.3.2 with Batch Normalisation & Dropout\n",
        "Best Results:\n",
        "\n",
        "---\n",
        "1. Dropout = 0.2: Poorer results | F1 Macro: 0.761905\t| Accuracy: 0.8625 \n",
        "2. Dropout = 0.15 Best results | F1 Macro: 0.787359 | Accuracy: 0.8906 | CV Acc: 0.9012\n",
        "3. Dropout = 0.12 Better results | F1 Macro: 0.7431 | Accuracy: 0.8656 | CV Acc: 0.8999\n",
        "---\n",
        "Since batch norm introduces noise and thus a slight regularisation effect whilst minimising covariate shift. We will reduce the dropout_rate to retain more information. (Tune with HyperModel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzDx5JOUq1SR"
      },
      "source": [
        "def build_bn_dropout_model(dropout=0.15):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Dense(128, input_dim=13, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout))\n",
        "\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout))\n",
        "\n",
        "  model.add(Dense(224, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(dropout))\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaUCPpLmuA7v"
      },
      "source": [
        "# Create Pipeline for BatchNorm\n",
        "pipe_bn_dropout = []\n",
        "pipe_bn_dropout.append(('standardize', StandardScaler()))\n",
        "pipe_bn_dropout.append(('mlp', KerasClassifier(build_fn=build_bn_dropout_model, epochs=200, batch_size=16, verbose=0)))\n",
        "pipeline_bn_dropout = Pipeline(pipe_bn_dropout)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2vTa4dWuOSz"
      },
      "source": [
        "# Train and Predict\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "results_bnorm = cross_val_score(pipeline_bn_dropout, final_df, target, cv=kfold)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj73JZbTuQk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dcc5d1f-909a-496f-b3c0-a4edce79c480"
      },
      "source": [
        "print(\"Baseline NN Classifier with Batch Normalisation has a mean cross val accuracy of {:.2f}%\".format(results_bnorm.mean()*100))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline NN Classifier with Batch Normalisation has a mean cross val accuracy of 90.31%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2jGg7DXyZl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce32452-99c4-4dcc-98dc-c71a09513118"
      },
      "source": [
        "# Dropout model & F1\n",
        "bn_dropout_model = build_bn_dropout_model()\n",
        "\n",
        "# Create call backs\n",
        "my_callbacks_bn_dropout = [\n",
        "                           EarlyStopping(monitor='val_accuracy', patience=50, mode='max'),\n",
        "                           TensorBoard(log_dir='./logs')\n",
        "                           ]\n",
        "\n",
        "# Train model\n",
        "bn_dropout_model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val), class_weight=weights, epochs=200, batch_size=16, callbacks=my_callbacks_bn_dropout)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            " 2/60 [>.............................] - ETA: 33s - loss: 1.9840 - accuracy: 0.4688WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0090s vs `on_train_batch_end` time: 1.1417s). Check your callbacks.\n",
            "60/60 [==============================] - 2s 27ms/step - loss: 1.3603 - accuracy: 0.6142 - val_loss: 0.6304 - val_accuracy: 0.6719\n",
            "Epoch 2/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 1.1369 - accuracy: 0.6674 - val_loss: 0.5606 - val_accuracy: 0.7469\n",
            "Epoch 3/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 1.0406 - accuracy: 0.6966 - val_loss: 0.5344 - val_accuracy: 0.7437\n",
            "Epoch 4/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.9537 - accuracy: 0.7195 - val_loss: 0.5187 - val_accuracy: 0.7625\n",
            "Epoch 5/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.8772 - accuracy: 0.7602 - val_loss: 0.5912 - val_accuracy: 0.7281\n",
            "Epoch 6/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.8127 - accuracy: 0.7487 - val_loss: 0.5866 - val_accuracy: 0.7188\n",
            "Epoch 7/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.7743 - accuracy: 0.7477 - val_loss: 0.5711 - val_accuracy: 0.7688\n",
            "Epoch 8/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.9249 - accuracy: 0.7570 - val_loss: 0.7528 - val_accuracy: 0.7344\n",
            "Epoch 9/200\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 0.7503 - accuracy: 0.7748 - val_loss: 0.5810 - val_accuracy: 0.7375\n",
            "Epoch 10/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.7502 - accuracy: 0.7758 - val_loss: 0.5945 - val_accuracy: 0.7437\n",
            "Epoch 11/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.7234 - accuracy: 0.7925 - val_loss: 0.6144 - val_accuracy: 0.7594\n",
            "Epoch 12/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6466 - accuracy: 0.8206 - val_loss: 0.6287 - val_accuracy: 0.7500\n",
            "Epoch 13/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.7833 - accuracy: 0.8019 - val_loss: 0.5753 - val_accuracy: 0.7594\n",
            "Epoch 14/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.7480 - accuracy: 0.7946 - val_loss: 0.5632 - val_accuracy: 0.7563\n",
            "Epoch 15/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.7197 - accuracy: 0.7946 - val_loss: 0.5552 - val_accuracy: 0.7531\n",
            "Epoch 16/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6676 - accuracy: 0.7956 - val_loss: 0.5550 - val_accuracy: 0.7469\n",
            "Epoch 17/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6497 - accuracy: 0.8217 - val_loss: 0.5147 - val_accuracy: 0.7750\n",
            "Epoch 18/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6646 - accuracy: 0.8165 - val_loss: 0.6159 - val_accuracy: 0.7688\n",
            "Epoch 19/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.7782 - accuracy: 0.8071 - val_loss: 0.6444 - val_accuracy: 0.7437\n",
            "Epoch 20/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6474 - accuracy: 0.8165 - val_loss: 0.5198 - val_accuracy: 0.7750\n",
            "Epoch 21/200\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 0.6750 - accuracy: 0.8279 - val_loss: 0.5173 - val_accuracy: 0.7688\n",
            "Epoch 22/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6063 - accuracy: 0.8227 - val_loss: 0.5271 - val_accuracy: 0.7844\n",
            "Epoch 23/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6447 - accuracy: 0.8133 - val_loss: 0.6205 - val_accuracy: 0.7344\n",
            "Epoch 24/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6555 - accuracy: 0.8175 - val_loss: 0.4936 - val_accuracy: 0.7750\n",
            "Epoch 25/200\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 0.5334 - accuracy: 0.8342 - val_loss: 0.4514 - val_accuracy: 0.8062\n",
            "Epoch 26/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5257 - accuracy: 0.8644 - val_loss: 0.4315 - val_accuracy: 0.8062\n",
            "Epoch 27/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6770 - accuracy: 0.8186 - val_loss: 0.5496 - val_accuracy: 0.7656\n",
            "Epoch 28/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5941 - accuracy: 0.8259 - val_loss: 0.5538 - val_accuracy: 0.7656\n",
            "Epoch 29/200\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 0.5467 - accuracy: 0.8373 - val_loss: 0.5015 - val_accuracy: 0.7750\n",
            "Epoch 30/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6467 - accuracy: 0.8144 - val_loss: 0.5500 - val_accuracy: 0.7844\n",
            "Epoch 31/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5670 - accuracy: 0.8384 - val_loss: 0.5238 - val_accuracy: 0.7812\n",
            "Epoch 32/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5228 - accuracy: 0.8551 - val_loss: 0.6067 - val_accuracy: 0.7812\n",
            "Epoch 33/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5100 - accuracy: 0.8571 - val_loss: 0.4718 - val_accuracy: 0.8094\n",
            "Epoch 34/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6256 - accuracy: 0.8540 - val_loss: 0.5925 - val_accuracy: 0.7688\n",
            "Epoch 35/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5143 - accuracy: 0.8519 - val_loss: 0.4717 - val_accuracy: 0.7875\n",
            "Epoch 36/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4493 - accuracy: 0.8728 - val_loss: 0.5043 - val_accuracy: 0.8062\n",
            "Epoch 37/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5167 - accuracy: 0.8686 - val_loss: 0.4109 - val_accuracy: 0.8313\n",
            "Epoch 38/200\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 0.5582 - accuracy: 0.8509 - val_loss: 0.4882 - val_accuracy: 0.7969\n",
            "Epoch 39/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6733 - accuracy: 0.8269 - val_loss: 0.5098 - val_accuracy: 0.7875\n",
            "Epoch 40/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4875 - accuracy: 0.8655 - val_loss: 0.5660 - val_accuracy: 0.8031\n",
            "Epoch 41/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5193 - accuracy: 0.8676 - val_loss: 0.5654 - val_accuracy: 0.7719\n",
            "Epoch 42/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4601 - accuracy: 0.8655 - val_loss: 0.4896 - val_accuracy: 0.8219\n",
            "Epoch 43/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5360 - accuracy: 0.8686 - val_loss: 0.5214 - val_accuracy: 0.7969\n",
            "Epoch 44/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5264 - accuracy: 0.8436 - val_loss: 0.4606 - val_accuracy: 0.8219\n",
            "Epoch 45/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5498 - accuracy: 0.8551 - val_loss: 0.4744 - val_accuracy: 0.8094\n",
            "Epoch 46/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4656 - accuracy: 0.8738 - val_loss: 0.4972 - val_accuracy: 0.8125\n",
            "Epoch 47/200\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 0.4709 - accuracy: 0.8738 - val_loss: 0.4972 - val_accuracy: 0.7844\n",
            "Epoch 48/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4636 - accuracy: 0.8613 - val_loss: 0.4476 - val_accuracy: 0.8125\n",
            "Epoch 49/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5195 - accuracy: 0.8582 - val_loss: 0.4156 - val_accuracy: 0.8250\n",
            "Epoch 50/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4771 - accuracy: 0.8738 - val_loss: 0.5030 - val_accuracy: 0.8188\n",
            "Epoch 51/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5133 - accuracy: 0.8655 - val_loss: 0.5010 - val_accuracy: 0.8094\n",
            "Epoch 52/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.6225 - accuracy: 0.8394 - val_loss: 0.5433 - val_accuracy: 0.7875\n",
            "Epoch 53/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4363 - accuracy: 0.8780 - val_loss: 0.5148 - val_accuracy: 0.8094\n",
            "Epoch 54/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4119 - accuracy: 0.8863 - val_loss: 0.4844 - val_accuracy: 0.8313\n",
            "Epoch 55/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3969 - accuracy: 0.8957 - val_loss: 0.4535 - val_accuracy: 0.8281\n",
            "Epoch 56/200\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 0.5384 - accuracy: 0.8686 - val_loss: 0.5027 - val_accuracy: 0.8062\n",
            "Epoch 57/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3819 - accuracy: 0.8884 - val_loss: 0.4149 - val_accuracy: 0.8375\n",
            "Epoch 58/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4498 - accuracy: 0.8801 - val_loss: 0.4430 - val_accuracy: 0.8219\n",
            "Epoch 59/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4214 - accuracy: 0.8822 - val_loss: 0.4636 - val_accuracy: 0.8094\n",
            "Epoch 60/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4532 - accuracy: 0.8853 - val_loss: 0.4814 - val_accuracy: 0.8250\n",
            "Epoch 61/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5006 - accuracy: 0.8801 - val_loss: 0.6274 - val_accuracy: 0.7844\n",
            "Epoch 62/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4201 - accuracy: 0.8968 - val_loss: 0.4672 - val_accuracy: 0.8125\n",
            "Epoch 63/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5170 - accuracy: 0.8728 - val_loss: 0.4567 - val_accuracy: 0.8125\n",
            "Epoch 64/200\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 0.4408 - accuracy: 0.8916 - val_loss: 0.4826 - val_accuracy: 0.8344\n",
            "Epoch 65/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4494 - accuracy: 0.8978 - val_loss: 0.4226 - val_accuracy: 0.8438\n",
            "Epoch 66/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3832 - accuracy: 0.8936 - val_loss: 0.5075 - val_accuracy: 0.8281\n",
            "Epoch 67/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3908 - accuracy: 0.8884 - val_loss: 0.5069 - val_accuracy: 0.8219\n",
            "Epoch 68/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3556 - accuracy: 0.9009 - val_loss: 0.4820 - val_accuracy: 0.8313\n",
            "Epoch 69/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5721 - accuracy: 0.8832 - val_loss: 0.5337 - val_accuracy: 0.8156\n",
            "Epoch 70/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4191 - accuracy: 0.8989 - val_loss: 0.5621 - val_accuracy: 0.7875\n",
            "Epoch 71/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3937 - accuracy: 0.8905 - val_loss: 0.5277 - val_accuracy: 0.8219\n",
            "Epoch 72/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4651 - accuracy: 0.8863 - val_loss: 0.4461 - val_accuracy: 0.8000\n",
            "Epoch 73/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3397 - accuracy: 0.9051 - val_loss: 0.4438 - val_accuracy: 0.8375\n",
            "Epoch 74/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3331 - accuracy: 0.9072 - val_loss: 0.4402 - val_accuracy: 0.8313\n",
            "Epoch 75/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3363 - accuracy: 0.9208 - val_loss: 0.5632 - val_accuracy: 0.8031\n",
            "Epoch 76/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4063 - accuracy: 0.9103 - val_loss: 0.6244 - val_accuracy: 0.7844\n",
            "Epoch 77/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4270 - accuracy: 0.8978 - val_loss: 0.6530 - val_accuracy: 0.7969\n",
            "Epoch 78/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3563 - accuracy: 0.9062 - val_loss: 0.4641 - val_accuracy: 0.8156\n",
            "Epoch 79/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3454 - accuracy: 0.9030 - val_loss: 0.4219 - val_accuracy: 0.8438\n",
            "Epoch 80/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3930 - accuracy: 0.9197 - val_loss: 0.4651 - val_accuracy: 0.8281\n",
            "Epoch 81/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4079 - accuracy: 0.9041 - val_loss: 0.4380 - val_accuracy: 0.8344\n",
            "Epoch 82/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3816 - accuracy: 0.8999 - val_loss: 0.4758 - val_accuracy: 0.8094\n",
            "Epoch 83/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3344 - accuracy: 0.9093 - val_loss: 0.4667 - val_accuracy: 0.8250\n",
            "Epoch 84/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3778 - accuracy: 0.9114 - val_loss: 0.5793 - val_accuracy: 0.8156\n",
            "Epoch 85/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2987 - accuracy: 0.9155 - val_loss: 0.5230 - val_accuracy: 0.8344\n",
            "Epoch 86/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3101 - accuracy: 0.9239 - val_loss: 0.5584 - val_accuracy: 0.8281\n",
            "Epoch 87/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4248 - accuracy: 0.9041 - val_loss: 0.6331 - val_accuracy: 0.8062\n",
            "Epoch 88/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4725 - accuracy: 0.8947 - val_loss: 0.6008 - val_accuracy: 0.8031\n",
            "Epoch 89/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3269 - accuracy: 0.9072 - val_loss: 0.4621 - val_accuracy: 0.8500\n",
            "Epoch 90/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3323 - accuracy: 0.9176 - val_loss: 0.5314 - val_accuracy: 0.8094\n",
            "Epoch 91/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3036 - accuracy: 0.9145 - val_loss: 0.4453 - val_accuracy: 0.8313\n",
            "Epoch 92/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3131 - accuracy: 0.9176 - val_loss: 0.5365 - val_accuracy: 0.8156\n",
            "Epoch 93/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3104 - accuracy: 0.9051 - val_loss: 0.5272 - val_accuracy: 0.8313\n",
            "Epoch 94/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4083 - accuracy: 0.9082 - val_loss: 0.5607 - val_accuracy: 0.8281\n",
            "Epoch 95/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3552 - accuracy: 0.9041 - val_loss: 0.4375 - val_accuracy: 0.8281\n",
            "Epoch 96/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3459 - accuracy: 0.9062 - val_loss: 0.4583 - val_accuracy: 0.8438\n",
            "Epoch 97/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3783 - accuracy: 0.9093 - val_loss: 0.4629 - val_accuracy: 0.8281\n",
            "Epoch 98/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3858 - accuracy: 0.9041 - val_loss: 0.4859 - val_accuracy: 0.8125\n",
            "Epoch 99/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2577 - accuracy: 0.9343 - val_loss: 0.4984 - val_accuracy: 0.8500\n",
            "Epoch 100/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3502 - accuracy: 0.8999 - val_loss: 0.5144 - val_accuracy: 0.8469\n",
            "Epoch 101/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3532 - accuracy: 0.9208 - val_loss: 0.4802 - val_accuracy: 0.8313\n",
            "Epoch 102/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3249 - accuracy: 0.9082 - val_loss: 0.4766 - val_accuracy: 0.8438\n",
            "Epoch 103/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3049 - accuracy: 0.9197 - val_loss: 0.4881 - val_accuracy: 0.8594\n",
            "Epoch 104/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3882 - accuracy: 0.8999 - val_loss: 0.4519 - val_accuracy: 0.8531\n",
            "Epoch 105/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2702 - accuracy: 0.9322 - val_loss: 0.5477 - val_accuracy: 0.8156\n",
            "Epoch 106/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2225 - accuracy: 0.9322 - val_loss: 0.4565 - val_accuracy: 0.8438\n",
            "Epoch 107/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2651 - accuracy: 0.9374 - val_loss: 0.4592 - val_accuracy: 0.8531\n",
            "Epoch 108/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2624 - accuracy: 0.9406 - val_loss: 0.5467 - val_accuracy: 0.8281\n",
            "Epoch 109/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2892 - accuracy: 0.9291 - val_loss: 0.5608 - val_accuracy: 0.8375\n",
            "Epoch 110/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4745 - accuracy: 0.8957 - val_loss: 0.6533 - val_accuracy: 0.7781\n",
            "Epoch 111/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3676 - accuracy: 0.9093 - val_loss: 0.6168 - val_accuracy: 0.8125\n",
            "Epoch 112/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2561 - accuracy: 0.9176 - val_loss: 0.5376 - val_accuracy: 0.8344\n",
            "Epoch 113/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3203 - accuracy: 0.9291 - val_loss: 0.5637 - val_accuracy: 0.8313\n",
            "Epoch 114/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4141 - accuracy: 0.8989 - val_loss: 0.4951 - val_accuracy: 0.8125\n",
            "Epoch 115/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3076 - accuracy: 0.9114 - val_loss: 0.4581 - val_accuracy: 0.8375\n",
            "Epoch 116/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3492 - accuracy: 0.9218 - val_loss: 0.4982 - val_accuracy: 0.8375\n",
            "Epoch 117/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2626 - accuracy: 0.9322 - val_loss: 0.4402 - val_accuracy: 0.8313\n",
            "Epoch 118/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2895 - accuracy: 0.9249 - val_loss: 0.5105 - val_accuracy: 0.8344\n",
            "Epoch 119/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3227 - accuracy: 0.9208 - val_loss: 0.4563 - val_accuracy: 0.8250\n",
            "Epoch 120/200\n",
            "60/60 [==============================] - 0s 5ms/step - loss: 0.2871 - accuracy: 0.9281 - val_loss: 0.4401 - val_accuracy: 0.8469\n",
            "Epoch 121/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2929 - accuracy: 0.9312 - val_loss: 0.5645 - val_accuracy: 0.8062\n",
            "Epoch 122/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4290 - accuracy: 0.9020 - val_loss: 0.6411 - val_accuracy: 0.7875\n",
            "Epoch 123/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2641 - accuracy: 0.9197 - val_loss: 0.5196 - val_accuracy: 0.8375\n",
            "Epoch 124/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2411 - accuracy: 0.9406 - val_loss: 0.5363 - val_accuracy: 0.8250\n",
            "Epoch 125/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3356 - accuracy: 0.9093 - val_loss: 0.4658 - val_accuracy: 0.8219\n",
            "Epoch 126/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3584 - accuracy: 0.9228 - val_loss: 0.6138 - val_accuracy: 0.8313\n",
            "Epoch 127/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3359 - accuracy: 0.9270 - val_loss: 0.5584 - val_accuracy: 0.8219\n",
            "Epoch 128/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2532 - accuracy: 0.9291 - val_loss: 0.5375 - val_accuracy: 0.8250\n",
            "Epoch 129/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2605 - accuracy: 0.9364 - val_loss: 0.4603 - val_accuracy: 0.8344\n",
            "Epoch 130/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3741 - accuracy: 0.9072 - val_loss: 0.5431 - val_accuracy: 0.8250\n",
            "Epoch 131/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2829 - accuracy: 0.9281 - val_loss: 0.4478 - val_accuracy: 0.8469\n",
            "Epoch 132/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2266 - accuracy: 0.9416 - val_loss: 0.4781 - val_accuracy: 0.8469\n",
            "Epoch 133/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2445 - accuracy: 0.9406 - val_loss: 0.4746 - val_accuracy: 0.8406\n",
            "Epoch 134/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2550 - accuracy: 0.9364 - val_loss: 0.4502 - val_accuracy: 0.8687\n",
            "Epoch 135/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3382 - accuracy: 0.9281 - val_loss: 0.5587 - val_accuracy: 0.8313\n",
            "Epoch 136/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2479 - accuracy: 0.9385 - val_loss: 0.5503 - val_accuracy: 0.8406\n",
            "Epoch 137/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2225 - accuracy: 0.9406 - val_loss: 0.5635 - val_accuracy: 0.8219\n",
            "Epoch 138/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2007 - accuracy: 0.9541 - val_loss: 0.5980 - val_accuracy: 0.8406\n",
            "Epoch 139/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3015 - accuracy: 0.9322 - val_loss: 0.5835 - val_accuracy: 0.8281\n",
            "Epoch 140/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2201 - accuracy: 0.9426 - val_loss: 0.5068 - val_accuracy: 0.8469\n",
            "Epoch 141/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3452 - accuracy: 0.9322 - val_loss: 0.5118 - val_accuracy: 0.8281\n",
            "Epoch 142/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2323 - accuracy: 0.9374 - val_loss: 0.4906 - val_accuracy: 0.8500\n",
            "Epoch 143/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2353 - accuracy: 0.9437 - val_loss: 0.5749 - val_accuracy: 0.8375\n",
            "Epoch 144/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2124 - accuracy: 0.9468 - val_loss: 0.5563 - val_accuracy: 0.8562\n",
            "Epoch 145/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.1721 - accuracy: 0.9572 - val_loss: 0.5341 - val_accuracy: 0.8375\n",
            "Epoch 146/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2211 - accuracy: 0.9510 - val_loss: 0.5272 - val_accuracy: 0.8500\n",
            "Epoch 147/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3826 - accuracy: 0.9468 - val_loss: 0.5723 - val_accuracy: 0.8375\n",
            "Epoch 148/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3272 - accuracy: 0.9333 - val_loss: 0.4182 - val_accuracy: 0.8438\n",
            "Epoch 149/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2991 - accuracy: 0.9228 - val_loss: 0.4943 - val_accuracy: 0.8562\n",
            "Epoch 150/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.1875 - accuracy: 0.9541 - val_loss: 0.5254 - val_accuracy: 0.8531\n",
            "Epoch 151/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2334 - accuracy: 0.9531 - val_loss: 0.6191 - val_accuracy: 0.8406\n",
            "Epoch 152/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2608 - accuracy: 0.9374 - val_loss: 0.5473 - val_accuracy: 0.8594\n",
            "Epoch 153/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2200 - accuracy: 0.9447 - val_loss: 0.4916 - val_accuracy: 0.8562\n",
            "Epoch 154/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.1807 - accuracy: 0.9447 - val_loss: 0.4318 - val_accuracy: 0.8656\n",
            "Epoch 155/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4280 - accuracy: 0.9072 - val_loss: 0.7580 - val_accuracy: 0.7937\n",
            "Epoch 156/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2073 - accuracy: 0.9406 - val_loss: 0.5497 - val_accuracy: 0.8281\n",
            "Epoch 157/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2214 - accuracy: 0.9458 - val_loss: 0.6406 - val_accuracy: 0.8188\n",
            "Epoch 158/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3097 - accuracy: 0.9176 - val_loss: 0.4565 - val_accuracy: 0.8406\n",
            "Epoch 159/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2274 - accuracy: 0.9437 - val_loss: 0.5470 - val_accuracy: 0.8313\n",
            "Epoch 160/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2215 - accuracy: 0.9479 - val_loss: 0.5501 - val_accuracy: 0.8469\n",
            "Epoch 161/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2514 - accuracy: 0.9364 - val_loss: 0.5976 - val_accuracy: 0.8281\n",
            "Epoch 162/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3110 - accuracy: 0.9281 - val_loss: 0.6853 - val_accuracy: 0.8156\n",
            "Epoch 163/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2176 - accuracy: 0.9447 - val_loss: 0.5981 - val_accuracy: 0.8250\n",
            "Epoch 164/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3420 - accuracy: 0.9312 - val_loss: 0.5072 - val_accuracy: 0.8469\n",
            "Epoch 165/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2256 - accuracy: 0.9416 - val_loss: 0.5409 - val_accuracy: 0.8281\n",
            "Epoch 166/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3037 - accuracy: 0.9353 - val_loss: 0.5896 - val_accuracy: 0.8219\n",
            "Epoch 167/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2395 - accuracy: 0.9343 - val_loss: 0.5858 - val_accuracy: 0.8375\n",
            "Epoch 168/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.1760 - accuracy: 0.9541 - val_loss: 0.5090 - val_accuracy: 0.8562\n",
            "Epoch 169/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2447 - accuracy: 0.9489 - val_loss: 0.5717 - val_accuracy: 0.8438\n",
            "Epoch 170/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2516 - accuracy: 0.9333 - val_loss: 0.4858 - val_accuracy: 0.8500\n",
            "Epoch 171/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.3009 - accuracy: 0.9406 - val_loss: 0.4620 - val_accuracy: 0.8687\n",
            "Epoch 172/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2248 - accuracy: 0.9374 - val_loss: 0.4602 - val_accuracy: 0.8562\n",
            "Epoch 173/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.1709 - accuracy: 0.9552 - val_loss: 0.5151 - val_accuracy: 0.8469\n",
            "Epoch 174/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.1704 - accuracy: 0.9614 - val_loss: 0.5369 - val_accuracy: 0.8531\n",
            "Epoch 175/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2483 - accuracy: 0.9552 - val_loss: 0.5626 - val_accuracy: 0.8406\n",
            "Epoch 176/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2376 - accuracy: 0.9468 - val_loss: 0.6242 - val_accuracy: 0.8156\n",
            "Epoch 177/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2274 - accuracy: 0.9395 - val_loss: 0.6420 - val_accuracy: 0.8375\n",
            "Epoch 178/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2567 - accuracy: 0.9385 - val_loss: 0.5720 - val_accuracy: 0.8125\n",
            "Epoch 179/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.1677 - accuracy: 0.9593 - val_loss: 0.4943 - val_accuracy: 0.8406\n",
            "Epoch 180/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2159 - accuracy: 0.9531 - val_loss: 0.4884 - val_accuracy: 0.8344\n",
            "Epoch 181/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.1682 - accuracy: 0.9656 - val_loss: 0.5858 - val_accuracy: 0.8406\n",
            "Epoch 182/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2876 - accuracy: 0.9416 - val_loss: 0.6696 - val_accuracy: 0.8344\n",
            "Epoch 183/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2177 - accuracy: 0.9458 - val_loss: 0.5453 - val_accuracy: 0.8531\n",
            "Epoch 184/200\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.2780 - accuracy: 0.9458 - val_loss: 0.4966 - val_accuracy: 0.8406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc1a46b70b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihokRW1V7768"
      },
      "source": [
        "# Load tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7f0n3Zoy7Af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "0484cedc-84a1-4e16-e65e-aaebeb08d5d9"
      },
      "source": [
        "# Get predictions and F1 score\n",
        "y_pred5 = bn_dropout_model.predict(X_test_scaled) > 0.5\n",
        "\n",
        "# F1 score\n",
        "pd.DataFrame(classification_report(y_test, y_pred5, output_dict=True))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>weighted avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.965251</td>\n",
              "      <td>0.459016</td>\n",
              "      <td>0.86875</td>\n",
              "      <td>0.712134</td>\n",
              "      <td>0.906718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.883392</td>\n",
              "      <td>0.756757</td>\n",
              "      <td>0.86875</td>\n",
              "      <td>0.820074</td>\n",
              "      <td>0.868750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.922509</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.86875</td>\n",
              "      <td>0.746969</td>\n",
              "      <td>0.881916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>283.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>0.86875</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>320.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    0          1  accuracy   macro avg  weighted avg\n",
              "precision    0.965251   0.459016   0.86875    0.712134      0.906718\n",
              "recall       0.883392   0.756757   0.86875    0.820074      0.868750\n",
              "f1-score     0.922509   0.571429   0.86875    0.746969      0.881916\n",
              "support    283.000000  37.000000   0.86875  320.000000    320.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}